{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN4aK8O/151w65pCMscfIUA"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **1. 사전 학습(Pre-training)**\n","* 원하는 자연어처리 작업을 수행하는 데이터셋으로 모델을 학습시키기 이전에 일반적인 데이터에 먼저 학습을 시키는 것\n","* 사전학습은 대량의 자연어 코퍼스를 미리 학습하여, 자연어 코퍼스 안에 포함된 일반화된 언어 특성들을 모델의 파라미터 안에 함축하는 방법\n","* 사전 학습한 모델의 parameter를 이용하여 모델을 초기화한 뒤(pre-training) 사전 학습된 모델을 실제 풀려고 하는 문제의 데이터에 학습시키는 것 보다 더 높은 성능을 가짐\n","* 예) 컴퓨터 비전에선 엄청난 크기의 이미지 데이터인 ImageNet에 pre-train된 모델의 parameter를 가져와서 fine-tuning 하는 방법을 많이 사용하고 NLP에서는 GPT와 BERT를 시작으로 language modeling(MLM, NTP)을 통한 사전 학습으로 여러가지 downstream task들에 대해서 높은 성능 향상을 보여줌\n"],"metadata":{"id":"alDCxt3jsoT0"}},{"cell_type":"markdown","source":["# **2. 전이 학습(Transfer Learning)**\n","* 데이터가 아주 많은 도메인에서 학습된 지식을 데이터가 적은 도메인으로 전이시키는 학습 방법론\n","* 대규모 데이터와 일반화된 목적 함수로 모델을 미리 학습한 후, 해당 모델의 파라미터로 내가 원하는 자연어처리 작업을 학습할 때 모델을 최적화하는 학습방법\n","* 보다 일반화된 모델의 지식을 다운스트림 작업을 수행하는 모델에 전이하는 방법으로 모델의 일반화된 언어 지식을 자연어처리 작업에 활용하는 일종의 지식 전이 과정\n","* 비슷한 특징을 지닌 데이터셋으로 유사한 문제를 해결하고자 한다면, 랜덤하게 초기화된 상태의 가중치 파라미터에서 최적화를 시작하기보다는 기존에 이미 훈련된 가중치 파라미터에서 조금만 바꿔서 새로운 문제를 풀게 하는 편이 쉬울 수 있음\n","\n","> 완전 다른 성격의 데이터셋 또는 다른 성격의 문제를 해결할 때 전의 학습은 도움이 되지 않을 가능성이 높음!"],"metadata":{"id":"3K7usY5UvE3L"}},{"cell_type":"markdown","source":["# **3. 파인 튜닝(Fine-tuning)**\n","* 사전 학습된 모델에 일반화된 지식을 활용하여 원하는 자연어처리 작업 (downstream task)을 학습하는 학습 방법\n","* 파인 튜닝으로 모델을 학습할 때에는 사전 학습된 모델의 모든 파라미터와 더불어 downstram task를 수행하기 위해 최소한의 파라미터를 추가해서 모델을 추가로 학습해야 함\n","* 딥러닝 모델의 중요한 성격 중 하나가 바로 데이터셋 내 특징을 스스로 학습한다는 점 -> 모델의 low-level 레이어일수록 일반적인 특징을 추출하도록 하는 학습이 이루어짐. 마지막 레이어에 가까울수록 특정 데이터셋이 나타날 수 있는 구체적인 특징을 추출하도록 학습이 이루어짐\n","* 언어도 모르는 상태에서 해당 자연어처리 작업을 학습하는 것과 언어를 어느정도 아는 상태에서 자연어처리 작업을 학습하는 것이 많은 차이가 있음"],"metadata":{"id":"qHTqg3D9vFsT"}},{"cell_type":"markdown","source":["### 3-1. 파인 튜닝(Fine-tuning) 세 가지 방법\n","* 모델 전체를 새로 학습\n","* 모델 전체를 Freezing(추가 layer)\n","* 모델의 일부를 Freezinf(layer tuning)"],"metadata":{"id":"7vMIr_Yhz0hF"}},{"cell_type":"markdown","source":["### 3-2. 모델 freezing\n","* 해당 모델의 레이어의 파라미터를 학습 과정중에 최적화하지 않겠다는 의미\n","* freezing 레이어의 파라미터는 학습 중이라도 원래 파라미터가 유지됨\n","* 해당 방법은 주로 transfer learnig을 할 때 주로 사용하는데, 사전학습 모델의 피처 정보를 온전히 유지하기 위해 사용함\n","* 경우에 따라 어떤 레이어를, 어느 정도의 비율로 freezing할지 결정해야 함"],"metadata":{"id":"oVIP4alxvFuK"}},{"cell_type":"markdown","source":["### 3-3. 상황에 따른 파인 튜닝 전략\n","* 데이터셋의 크기가 크고 사전학습 때 사용한 데이터셋과 유사성이 작을 때\n","    * 전체 모델을 새로 학습하는 편이 좋음\n","    * 데이터셋의 크기가 크기 때문에, 모델의 파라미터를 모두 사용해도 해당 데이터셋에 overfitting 되지 않을 확률이 높음\n","    * 모델의 파라미터를 많이 사용할수록 큰 데이터셋의 유효한 특징들을 잘 추출할 수 있음\n","* 데이터셋의 크기가 크고 사전학습 때 사용한 데이터셋과의 유사성이 높을때\n","    * 사전 학습된 모델의 일부를 freezing하고 남은 레이어를 학습시키는 전략이 좋음\n","    * 데이터의 유사도가 높기 때문에 사전 학습된 모델이 이미 학습한 일반화된 지식을 충분히 활용할 수 있음\n","    * 사전 학습된 모델의 레이어를 모두 학습하는 것이 오히려 일반화된 지식활용을 방해할 수 있음\n","* 데이터셋의 크기가 작고 사전학습 때 사용한 데이터셋과의 유사성이 작을때\n","    * 유사성이 작더라도 데이터셋의 크기가 작기 때문에 사전학습한 모델의 일반화된 지식을 활용하기를 기대해야 함\n","    * 사전 학습된 모델의 일부를 freezing하고 남은 레이어를 학습시키는 전략이 유효\n","    * 모델을 freezing하지 않는다면 데이터셋이 너무 작기 때문에 쉽게 overfitting이 발생\n","* 데이터셋의 크기가 작고 사전학습 때 사용한 데이터셋과의 유사성이 높을때\n","    * 데이터셋이 작지만 유사성이 높기 때문에 모델 전체를 freezing하고 새로운 레이어를 추가하여 학습하는 전랻이 효과적\n","    * 데이터셋 간 유사성이 높지만 데이터셋이 작기 때문에 사전학습한 모델이 이미 학습한 지식을 최대한 활용하고 추가로 학습할 파라미터수를 최소화하는 식으로 전략을 설정\n","    * 성능 비교를 위해 사전학습한 모델의 레이어 일부를 점점 unfreezing하는 전략들을 시도할 수 있음"],"metadata":{"id":"q6CpoHuz2ZXO"}},{"cell_type":"markdown","source":["### 3-4. 최신 모델 tuning\n","* Adapter tuning\n","    * Free-forward layer 다음에 추가될 Adapter를 추가하여 해당 layer만 학습\n","    * 원하는 자연어처리 작업을 수행할 때 Transformer의 나머지 파라미터를 freezing하고 추가된 Adapter만 학습\n","    * 서로 다른 task를 수행할 때 해당 task에서 학습한 Adapter를 바꿔끼면 해당 Task를 수행\n","    * [논문](https://arxiv.org/pdf/1902.00751): https://arxiv.org/pdf/1902.00751\n","* Pre-fix tuning\n","    * Transformer에는 일련의 워드 임베딩 토큰이 입력으로 들어가는데, 해당 입력의 prefix에 고정된 개수의 embedding 토큰을 일종의 모듈처럼 입력으로 추가함\n","    * Transformer의 모든 파라미터를 freezing하고, prefix만 tuning\n","    * 각 task별로 prefix만 다르게 제공하더라도, 해당 작업을 잘 수행할 수 있음\n","    * [논문](https://arxiv.org/abs/2101.00190): https://arxiv.org/abs/2101.00190\n","* in-context learning\n","    * 모델의 크기가 충분히 커서 일반화 수준이 높다면 파인 튜닝 없이, 프롬프트 조작만으로 작업 수행\n","    * 2020년 GPT-3 논문에서 제안된 방법\n","    * [논문](https://arxiv.org/abs/2005.14165): https://arxiv.org/abs/2005.14165\n","* FL-tuning\n","    * Feed Forward Network(FFN)의 일부를 학습하는 식으로 학습을 효율화\n","    * 2022 [논문](https://arxiv.org/abs/2206.15312)에서 제안: https://arxiv.org/abs/2206.15312\n","    * 대부분의 NLU tasks에서 prompt-tuning을 이김"],"metadata":{"id":"ZQwGGXUb2ZZM"}},{"cell_type":"markdown","source":["### 3-5. 파인 듀닝의 불안정성\n","* BERT와 같은 Transformer 기반의 pretained 모델을 fine-tuning하는 것은 매우 불안정하다고 알려져 있음\n","* [논문](https://arxiv.org/abs/2006.04884)은 파인 튜닝의 불안정성의 원인을 밝히는데 집중, 파인 튜닝을 수행하면 모델이 학습 데이터의 부정확한 패턴이나 편향을 학습할 뿐만 아니라. 불안정한 학습을 하는 경우가 있음\n","    * https://arxiv.org/abs/2006.04884\n","    * 불안정한 학습: 같은 데이터셋에 대해서 같은 모델로 학습할 때, 학습에 사용하는 램덤 시드를 바꿔가면서 여러 번 파인 튜닝했을 때 보여주는 모델의 성능 간 편차가 매우 큰 상황\n","* Catastrophic forgetting\n","    * 하나의 신경망 네트워크가 두가지 서로 다른 task에 대해 순차적으로 학습될 때 발생\n","    * 첫번째 task를 수행하는 능력을 두번째 task에 대해 학습할 때 잃어버리게 됨\n","* Small Size of the Fine-tuning Dataset\n","    * 작은 학습 데이터셋은 Fine-tuning 불안정성을 설명하는 가장 많이 알려진 가설로 학습 데이터셋과 연관이 있다고 BERT 연구에서 말하고 있음"],"metadata":{"id":"G3fetoJ62ZbP"}},{"cell_type":"markdown","source":["### 3-6. 학습의 불안정성을 해경하는 방법\n","* 하이퍼파라미터 튜닝을 이용한 해결\n","    * hidden layer의 차원수\n","    * dropout 비율\n","        * 모델 레이어 내 노드 중 일부가 학습할 때마다 무작위로 제거되는 방식으로 일반화 성능을 높이는 방법\n","    * 활성화 함수\n","    * learning rate\n","        * 학습 초기에 learning rate를 작게 설정해서 학습의 안정성을 확보한 뒤, 학습할 수록 learning rate를 크게 조정할 수 있음\n","    * Optimizer\n","    * loss function\n","    * 학습량(epoch, batch size)"],"metadata":{"id":"d7uNJy162ZdM"}},{"cell_type":"markdown","source":[],"metadata":{"id":"FPkzNBHjRnt1"}}]}