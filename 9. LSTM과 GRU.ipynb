{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["RNN"],"metadata":{"id":"tiIadqBeFUSG"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import numpy as np\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.feature_extraction.text import CountVectorizer\n","from torch.utils.data import DataLoader, Dataset\n","from sklearn.datasets import fetch_20newsgroups\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score"],"metadata":{"id":"g5laSn2RIdrp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# sklearn.datasets.fetch_20newsgroups를 사용하여 데이터 가져오기\n","newsgroups_data = fetch_20newsgroups(subset='all')\n","texts, labels = newsgroups_data.data, newsgroups_data.target"],"metadata":{"id":"uxa0GmfOI3Tg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["labels"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jNVKRMDGSkyw","executionInfo":{"status":"ok","timestamp":1719899490977,"user_tz":-540,"elapsed":3,"user":{"displayName":"Heeseon Im","userId":"07947113443531630188"}},"outputId":"6defd7f3-b877-4cf1-aeec-e595cd348ce1"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([10,  3, 17, ...,  3,  1,  7])"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["# 데이터를 학습: 80%, 테스트: 20% 로 나누기\n","label_encoder = LabelEncoder()\n","labels = label_encoder.fit_transform(labels)\n","\n","X_train, X_test, y_train, y_test = train_test_split(texts, labels, test_size=0.2, random_state=2024)"],"metadata":{"id":"9y57G5SgVbW8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 테스트 데이터를 CountVectorizer를 사용하여 벡터화\n","# 텍스트 데이터를 벡터화\n","# r'\\b\\w+\\b'\n","# r: \\해석하지 않게 함\n","# \\b: 단어의 시작 또는 끝을 의미\n","# \\w: 단어 문자를 의미\n","vectorizer = CountVectorizer(max_features=10000, token_pattern=r'\\b\\w+\\b')\n","X_train = vectorizer.fit_transform(X_train).toarray()\n","X_test = vectorizer.transform(X_test).toarray()"],"metadata":{"id":"bckUu1FRVfmE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 파이토치 텐서로 변환\n","X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n","X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n","y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n","y_test_tensor = torch.tensor(y_test, dtype=torch.long)"],"metadata":{"id":"A3CE1NmfJJg0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 데이터셋 클래스 정의\n","class NewsGroupDataset(Dataset):\n","    def __init__(self, X, y):\n","        self.X = X\n","        self.y = y\n","\n","    def __len__(self):\n","        return len(self.X)\n","\n","    def __getitem__(self, idx):\n","        return self.X[idx], self.y[idx]"],"metadata":{"id":"2Sjhq_NEVn47"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_dataset = NewsGroupDataset(X_train_tensor, y_train_tensor)\n","test_dataset = NewsGroupDataset(X_test_tensor, y_test_tensor)"],"metadata":{"id":"L-A9A6eJVry8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(train_dataset)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KsE88gZPV3A_","executionInfo":{"status":"ok","timestamp":1719899499445,"user_tz":-540,"elapsed":4,"user":{"displayName":"Heeseon Im","userId":"07947113443531630188"}},"outputId":"28338e0c-effe-40a0-ac1d-d532a7a5b0f4"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["15076"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["train_dataset[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yRc_RzAgV4gb","executionInfo":{"status":"ok","timestamp":1719899499445,"user_tz":-540,"elapsed":3,"user":{"displayName":"Heeseon Im","userId":"07947113443531630188"}},"outputId":"fc4d5a95-4940-4dc3-a886-a3122d67e932"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([0., 0., 0.,  ..., 0., 0., 0.]), tensor(11))"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["# 데이터로더 생성\n","train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"],"metadata":{"id":"1c1N4LQJV6jq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# RNN 모델\n","class RNNModel(nn.Module):\n","    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n","        super(RNNModel, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n","        self.fc = nn.Linear(hidden_size, output_size)\n","\n","    def forward(self, x):\n","        h = torch.zeros(1, x.size(0), self.hidden_size).to(x.device)\n","        out, _ = self.rnn(x, h)\n","        out = self.fc(out[:, -1, :])\n","        return out\n","\n","input_size = 10000\n","hidden_size = 128\n","output_size = len(label_encoder.classes_)\n","num_layers = 1\n","\n","model = RNNModel(input_size, hidden_size, output_size, num_layers)\n","model = model.to('cuda' if torch.cuda.is_available() else 'cpu')"],"metadata":{"id":"_eeSwbnGVqnu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 학습\n","loss_fun = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)"],"metadata":{"id":"2rDW8cfJWTam"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["num_epochs = 10\n","\n","for epoch in range(num_epochs):\n","    model.train()\n","    for X_batch, y_batch in train_loader:\n","        X_batch = X_batch.unsqueeze(1)\n","        # X_batch, y_batch = X_batch.to(model.device), y_batch.to(model.device)\n","        outputs = model(X_batch)\n","        loss = loss_fun(outputs, y_batch)\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","    print(f'Epoch: {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6NqOJFgebIde","executionInfo":{"status":"ok","timestamp":1719899550121,"user_tz":-540,"elapsed":47911,"user":{"displayName":"Heeseon Im","userId":"07947113443531630188"}},"outputId":"4b6f9ff4-9cb2-4651-f192-99cb78329e1f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 1/10, Loss: 0.4655\n","Epoch: 2/10, Loss: 0.1516\n","Epoch: 3/10, Loss: 0.1334\n","Epoch: 4/10, Loss: 0.0395\n","Epoch: 5/10, Loss: 0.0138\n","Epoch: 6/10, Loss: 0.0211\n","Epoch: 7/10, Loss: 0.0147\n","Epoch: 8/10, Loss: 0.0037\n","Epoch: 9/10, Loss: 0.0033\n","Epoch: 10/10, Loss: 0.0029\n"]}]},{"cell_type":"code","source":["# 모델평가\n","model.eval()\n","y_test, y_pred = [], []\n","\n","with torch.no_grad():\n","    for X_batch, y_batch in test_loader:\n","        X_batch = X_batch.unsqueeze(1)\n","        outputs = model(X_batch)\n","        _, pred = torch.max(outputs, 1)\n","        y_test.extend(y_batch.detach().numpy())\n","        y_pred.extend(pred.detach().numpy())\n","\n","accuracy = accuracy_score(y_test, y_pred)\n","print(f'accuracy: {accuracy:.4f}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nXCOw7NrbspL","executionInfo":{"status":"ok","timestamp":1719899550121,"user_tz":-540,"elapsed":3,"user":{"displayName":"Heeseon Im","userId":"07947113443531630188"}},"outputId":"49052882-d9ea-4446-ab97-d102f9829277"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["accuracy: 0.9021\n"]}]},{"cell_type":"markdown","source":["### RNN (Recurrent Neural Network)\n","* 장점\n","    * 시퀀스 데이터의 시간적 의존성을 모델링할 수 있음.\n","* 단점\n","    * 입력과 출력이 고정\n","    * 긴 시퀀스의 경우 기울기 소실 문제(Vanishing Gradient Problem) 발생\n","    * 단점을 극복하기 위해 RNN의 발전 형태인 LSTM(Long Short-Term Memory)과 GRU(Gated Recurrent Unit)를 사용(문제를 완벽히 해결하지 못함)"],"metadata":{"id":"aXWK2hieHM1C"}},{"cell_type":"markdown","source":["# **1. LSTM(Long Short-Term Memory)**\n","* 바닐라 RNN은 시퀀스 데이터를 처리할 때 시간이 지남에 따라 정보가 소실되거나 기울기 소실 문제가 발생\n","* 순환 신경망(RNN)의 한 종류로, 긴 시퀀스 데이터를 효과적으로 학습할 수 있도록 고안된 구조\n","* 참고 : https://wikidocs.net/22888"],"metadata":{"id":"zzPw1ycdFtC-"}},{"cell_type":"markdown","source":["### 1-1. LSTM의 구조\n","* 입력 게이트: 현재 입력값과 이전의 은닉 상태를 사용하여 어떤 정보를 새롭게 저장할지 결정\n","* 망각 게이트: 현재 입력값과 이전의 은닉 상태를 사용하여 어떤 정보를 잊을지 결정\n","* 출력 게이트: 현재 입력값과 이전의 은닉 상태를 사용하여 다음 은닉 상태를 결정\n","* 셀 상태: 정보가 직접 흐르는 경로로, 정보가 소실되지 않도록 함"],"metadata":{"id":"YzpaIF38Gfq2"}},{"cell_type":"markdown","source":["### 1-2. LSTM으로 예제 변환"],"metadata":{"id":"hCDzGFvhIbId"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import numpy as np\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.feature_extraction.text import CountVectorizer\n","from torch.utils.data import DataLoader, Dataset\n","from sklearn.datasets import fetch_20newsgroups\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","\n","newsgroups_data = fetch_20newsgroups(subset='all')\n","texts, labels = newsgroups_data.data, newsgroups_data.target\n","\n","label_encoder = LabelEncoder()\n","labels = label_encoder.fit_transform(labels)\n","\n","X_train, X_test, y_train, y_test = train_test_split(texts, labels, test_size=0.2, random_state=2024)\n","\n","vectorizer = CountVectorizer(max_features=10000, token_pattern=r'\\b\\w+\\b')\n","X_train = vectorizer.fit_transform(X_train).toarray()\n","X_test = vectorizer.transform(X_test).toarray()\n","\n","X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n","X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n","y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n","y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n","\n","class NewsGroupDataset(Dataset):\n","    def __init__(self, X, y):\n","        self.X = X\n","        self.y = y\n","\n","    def __len__(self):\n","        return len(self.X)\n","\n","    def __getitem__(self, idx):\n","        return self.X[idx], self.y[idx]\n","\n","train_dataset = NewsGroupDataset(X_train_tensor, y_train_tensor)\n","test_dataset = NewsGroupDataset(X_test_tensor, y_test_tensor)\n","\n","train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n","\n","class LSTMModel(nn.Module):\n","    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n","        super(LSTMModel, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n","        self.fc = nn.Linear(hidden_size, output_size)\n","\n","    def forward(self, x):\n","        h = torch.zeros(1, x.size(0), self.hidden_size).to(x.device)\n","        c = torch.zeros(1, x.size(0), self.hidden_size).to(x.device)\n","        out, _ = self.lstm(x, (h, c))\n","        out = self.fc(out[:, -1, :])\n","        return out\n","\n","input_size = 10000\n","hidden_size = 128\n","output_size = len(label_encoder.classes_)\n","num_layers = 1\n","\n","model = LSTMModel(input_size, hidden_size, output_size, num_layers)\n","model = model.to('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","loss_fun = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","num_epochs = 10\n","\n","for epoch in range(num_epochs):\n","    model.train()\n","    for X_batch, y_batch in train_loader:\n","        X_batch = X_batch.unsqueeze(1)\n","        # X_batch, y_batch = X_batch.to(model.device), y_batch.to(model.device)\n","        outputs = model(X_batch)\n","        loss = loss_fun(outputs, y_batch)\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","    print(f'Epoch: {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}')\n","\n","\n","model.eval()\n","y_test, y_pred = [], []\n","\n","with torch.no_grad():\n","    for X_batch, y_batch in test_loader:\n","        X_batch = X_batch.unsqueeze(1)\n","        outputs = model(X_batch)\n","        _, pred = torch.max(outputs, 1)\n","        y_test.extend(y_batch.detach().numpy())\n","        y_pred.extend(pred.detach().numpy())\n","\n","accuracy = accuracy_score(y_test, y_pred)\n","print(f'accuracy: {accuracy:.4f}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Epv3kDot3kfU","executionInfo":{"status":"ok","timestamp":1719899916826,"user_tz":-540,"elapsed":366706,"user":{"displayName":"Heeseon Im","userId":"07947113443531630188"}},"outputId":"babf0e18-bbdc-43fb-fd88-26af1664eb17"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 1/10, Loss: 0.4688\n","Epoch: 2/10, Loss: 0.1226\n","Epoch: 3/10, Loss: 0.1496\n","Epoch: 4/10, Loss: 0.0200\n","Epoch: 5/10, Loss: 0.0120\n","Epoch: 6/10, Loss: 0.0084\n","Epoch: 7/10, Loss: 0.0066\n","Epoch: 8/10, Loss: 0.0030\n","Epoch: 9/10, Loss: 0.0033\n","Epoch: 10/10, Loss: 0.0041\n","accuracy: 0.9013\n"]}]},{"cell_type":"markdown","source":["# **2. GRU(Gated Recurrent Unit)**\n","* LSTM과 유사하지만 구조가 더 간단한 RNN의 한 종류\n","* LSTM과 달리 셀 상태(cell state)를 가지지 않으며, 업데이트 게이트와 리셋 게이트를 사용하여 정보를 처리\n","* 참고: https://wikidocs.net/22889"],"metadata":{"id":"azH3xO2S41QV"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import numpy as np\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.feature_extraction.text import CountVectorizer\n","from torch.utils.data import DataLoader, Dataset\n","from sklearn.datasets import fetch_20newsgroups\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","\n","newsgroups_data = fetch_20newsgroups(subset='all')\n","texts, labels = newsgroups_data.data, newsgroups_data.target\n","\n","label_encoder = LabelEncoder()\n","labels = label_encoder.fit_transform(labels)\n","\n","X_train, X_test, y_train, y_test = train_test_split(texts, labels, test_size=0.2, random_state=2024)\n","\n","vectorizer = CountVectorizer(max_features=10000, token_pattern=r'\\b\\w+\\b')\n","X_train = vectorizer.fit_transform(X_train).toarray()\n","X_test = vectorizer.transform(X_test).toarray()\n","\n","X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n","X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n","y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n","y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n","\n","class NewsGroupDataset(Dataset):\n","    def __init__(self, X, y):\n","        self.X = X\n","        self.y = y\n","\n","    def __len__(self):\n","        return len(self.X)\n","\n","    def __getitem__(self, idx):\n","        return self.X[idx], self.y[idx]\n","\n","train_dataset = NewsGroupDataset(X_train_tensor, y_train_tensor)\n","test_dataset = NewsGroupDataset(X_test_tensor, y_test_tensor)\n","\n","train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n","\n","class GRUModel(nn.Module):\n","    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n","        super(GRUModel, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n","        self.fc = nn.Linear(hidden_size, output_size)\n","\n","    def forward(self, x):\n","        h = torch.zeros(1, x.size(0), self.hidden_size).to(x.device)\n","        out, _ = self.gru(x, h)\n","        out = self.fc(out[:, -1, :])\n","        return out\n","\n","input_size = 10000\n","hidden_size = 128\n","output_size = len(label_encoder.classes_)\n","num_layers = 1\n","\n","model = GRUModel(input_size, hidden_size, output_size, num_layers)\n","model = model.to('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","loss_fun = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","num_epochs = 10\n","\n","for epoch in range(num_epochs):\n","    model.train()\n","    for X_batch, y_batch in train_loader:\n","        X_batch = X_batch.unsqueeze(1)\n","        # X_batch, y_batch = X_batch.to(model.device), y_batch.to(model.device)\n","        outputs = model(X_batch)\n","        loss = loss_fun(outputs, y_batch)\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","    print(f'Epoch: {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}')\n","\n","\n","model.eval()\n","y_test, y_pred = [], []\n","\n","with torch.no_grad():\n","    for X_batch, y_batch in test_loader:\n","        X_batch = X_batch.unsqueeze(1)\n","        outputs = model(X_batch)\n","        _, pred = torch.max(outputs, 1)\n","        y_test.extend(y_batch.detach().numpy())\n","        y_pred.extend(pred.detach().numpy())\n","\n","accuracy = accuracy_score(y_test, y_pred)\n","print(f'accuracy: {accuracy:.4f}')"],"metadata":{"id":"aGxLbbXl7XQd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1719900048550,"user_tz":-540,"elapsed":131736,"user":{"displayName":"Heeseon Im","userId":"07947113443531630188"}},"outputId":"26bf25a0-761b-437e-bc02-4c9b8d25ba7f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 1/10, Loss: 0.2912\n","Epoch: 2/10, Loss: 0.3968\n","Epoch: 3/10, Loss: 0.0395\n","Epoch: 4/10, Loss: 0.0150\n","Epoch: 5/10, Loss: 0.0085\n","Epoch: 6/10, Loss: 0.0059\n","Epoch: 7/10, Loss: 0.0060\n","Epoch: 8/10, Loss: 0.0112\n","Epoch: 9/10, Loss: 0.0683\n","Epoch: 10/10, Loss: 0.0017\n","accuracy: 0.8992\n"]}]},{"cell_type":"markdown","source":["# **3. LSTM vs GRU**\n","* LSTM과 GRU는 RNN의 기울기 소실 단점을 해결하기 위해 고안\n","* 게이트 메커니즘을 사용하여 중요한 정보를 유지하고 불필요한 정보를 제거\n","* 긴 시퀀스를 효과적으로 처리할 수 있어 많은 자연어 처리 작업에서 사용\n","* LSTM\n","    * 게이트 수: 3개(입력, 망각, 출력)\n","    * 셀 상태를 유지\n","    * 구조가 복잡함\n","    * 매개변수가 GRU보다 많음\n","    * 훈련시간이 GRU보다 오래 걸림\n","* GRU\n","    * 게스트 수: 2개(업데이트, 리셋)\n","    * 셀 상태가 없음\n","    * 구조는 단순함\n","    * 매개변수가 LSTM보다 작음\n","    * 훈련시간이 LSTM보다 적게 걸림"],"metadata":{"id":"FipKJe5I7xe1"}},{"cell_type":"markdown","source":["### 과제\n","* 네이버 쇼핑 리뷰 감성 분류하기\n","* 단, 모델은 LSTM 또는 GRU를 사용, 파이토치를 이용\n","* 참고: https://wikidocs.net/94600"],"metadata":{"id":"nJ9n6Gmt9n1C"}},{"cell_type":"markdown","source":["### 논문리뷰\n","* Sequence to Sequence Learning with Neural Networks\n","    * https://arxiv.org/abs/1409.3215\n","* Neural Machine Translation by Jointly Learning to Align and Translate\n","    * https://arxiv.org/abs/1409.0473\n","* Effective Approaches to Attention-based Neural Machine Translation\n","    * https://arxiv.org/abs/1508.04025"],"metadata":{"id":"alEyueyeKJX4"}},{"cell_type":"code","source":[],"metadata":{"id":"KTopMRyz9wnC"},"execution_count":null,"outputs":[]}]}