{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"d9d9030f78fd4d5cb8c21db88b9cad12":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e2b94fcbc633415989b3e01022c3a6dc","IPY_MODEL_ef7cd9250ec145b191617660452765e3","IPY_MODEL_8bed00e751ac4742ad942f64309cf27c"],"layout":"IPY_MODEL_1c8e69027a564803827ff0815c569346"}},"e2b94fcbc633415989b3e01022c3a6dc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c7e69a98c3a7405fb7d152bd96652d9f","placeholder":"​","style":"IPY_MODEL_0c24bef00b664229b485e3ff4acba495","value":"config.json: 100%"}},"ef7cd9250ec145b191617660452765e3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e38ad5696b334c27bad23969f2b7f212","max":426,"min":0,"orientation":"horizontal","style":"IPY_MODEL_11cb954ed830479dbda9a484a5b68072","value":426}},"8bed00e751ac4742ad942f64309cf27c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_85907bd2122f4660b51e34c6699931f7","placeholder":"​","style":"IPY_MODEL_413de357a37a46a6a8f4d8c55f8df2dd","value":" 426/426 [00:00&lt;00:00, 8.17kB/s]"}},"1c8e69027a564803827ff0815c569346":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c7e69a98c3a7405fb7d152bd96652d9f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0c24bef00b664229b485e3ff4acba495":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e38ad5696b334c27bad23969f2b7f212":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"11cb954ed830479dbda9a484a5b68072":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"85907bd2122f4660b51e34c6699931f7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"413de357a37a46a6a8f4d8c55f8df2dd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"88a359e344fc457dae489fd0cfcde0a3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c53f8a4eb69041318fc41842c1040c4c","IPY_MODEL_982d122e04bd4c77b8a750c34e79a520","IPY_MODEL_ccb7a08534634f0ea7f9990fbefe25f2"],"layout":"IPY_MODEL_4db6310ce7464c65b95e134d14560128"}},"c53f8a4eb69041318fc41842c1040c4c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3d237a73bc0648adad6b109967eb61d8","placeholder":"​","style":"IPY_MODEL_d8386917c06e4544b040888e357c6279","value":"model.safetensors: 100%"}},"982d122e04bd4c77b8a750c34e79a520":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_46870319d44c40d990c64f094ddceaa4","max":368769812,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c13c3bf8334b41b491deb6079bffdd54","value":368769812}},"ccb7a08534634f0ea7f9990fbefe25f2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_843fbd8d19f541c0b44276c4731d8835","placeholder":"​","style":"IPY_MODEL_3ae282ab245348c98a8aaa1e262f8be9","value":" 369M/369M [00:03&lt;00:00, 113MB/s]"}},"4db6310ce7464c65b95e134d14560128":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3d237a73bc0648adad6b109967eb61d8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d8386917c06e4544b040888e357c6279":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"46870319d44c40d990c64f094ddceaa4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c13c3bf8334b41b491deb6079bffdd54":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"843fbd8d19f541c0b44276c4731d8835":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3ae282ab245348c98a8aaa1e262f8be9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"53999cc586154e5e982c66c2a4cd314f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a3b81e2a995d452b95edfaf2956e9b48","IPY_MODEL_438c1ffc01894509aff17cfc5962fa12","IPY_MODEL_0d62360e2e0642f993a8416f9a1bce95"],"layout":"IPY_MODEL_1a8d6e73e0a14c2a8619968c3022630d"}},"a3b81e2a995d452b95edfaf2956e9b48":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7fbe2c531f02490d907481ecf9f41028","placeholder":"​","style":"IPY_MODEL_ba68c8cd5e4647bdabd062c216f43d06","value":"tokenizer_config.json: 100%"}},"438c1ffc01894509aff17cfc5962fa12":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ee34bfc8b7bf4539bd83482257154323","max":432,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1a60267dd4104c4b882237cb11a42e24","value":432}},"0d62360e2e0642f993a8416f9a1bce95":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f5b8ca4c4c4d4a84852d6d15fee31a0a","placeholder":"​","style":"IPY_MODEL_f2379e727569412ebe4389466ccfcdcb","value":" 432/432 [00:00&lt;00:00, 3.29kB/s]"}},"1a8d6e73e0a14c2a8619968c3022630d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7fbe2c531f02490d907481ecf9f41028":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ba68c8cd5e4647bdabd062c216f43d06":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ee34bfc8b7bf4539bd83482257154323":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1a60267dd4104c4b882237cb11a42e24":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f5b8ca4c4c4d4a84852d6d15fee31a0a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f2379e727569412ebe4389466ccfcdcb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d7cc6c1883cc4afdafe792a66b592262":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a44910377c2045089fdc7b3271cf5dc2","IPY_MODEL_3086217282324597b4f5225fc7b36f27","IPY_MODEL_9e0a64701699436680ea7c06a9a5d00a"],"layout":"IPY_MODEL_b75016c391e6446ebd754d8305482fed"}},"a44910377c2045089fdc7b3271cf5dc2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_573d5fe450bc45378f1b41b9fa00885e","placeholder":"​","style":"IPY_MODEL_0d2950880f9c4dd5ba6b71ad8f8e07d1","value":"spiece.model: 100%"}},"3086217282324597b4f5225fc7b36f27":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_06a8483d6de8406ba4a3448d208fa22f","max":371427,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2a844c5ee31b4774a27bc5fadd5f5185","value":371427}},"9e0a64701699436680ea7c06a9a5d00a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_09f0cfe0269f45b1b37b67419d518d71","placeholder":"​","style":"IPY_MODEL_f9c1ab4497234ce5b42df1b64e98b74b","value":" 371k/371k [00:00&lt;00:00, 2.19MB/s]"}},"b75016c391e6446ebd754d8305482fed":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"573d5fe450bc45378f1b41b9fa00885e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0d2950880f9c4dd5ba6b71ad8f8e07d1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"06a8483d6de8406ba4a3448d208fa22f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2a844c5ee31b4774a27bc5fadd5f5185":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"09f0cfe0269f45b1b37b67419d518d71":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f9c1ab4497234ce5b42df1b64e98b74b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5c21506eb8eb4511a1bf52cab78b0bd6":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ec03c71d8c7d4130b6f5e854fdb2a11d","IPY_MODEL_bde2727457bb4feaab6b7d3ac4a7a4b1","IPY_MODEL_56ad9e9a9c014bb9b04699296861bdd9"],"layout":"IPY_MODEL_b84e111f2b354895be518d2c2fe861ac"}},"ec03c71d8c7d4130b6f5e854fdb2a11d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b0c8635fce134b159ff0803b0d7737a8","placeholder":"​","style":"IPY_MODEL_b865426eba144607a0605ca32f183802","value":"special_tokens_map.json: 100%"}},"bde2727457bb4feaab6b7d3ac4a7a4b1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c698040f165144ad878e04535e3df9b3","max":244,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c33f3518c5cc4c2286f67ecc4377233f","value":244}},"56ad9e9a9c014bb9b04699296861bdd9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d3b0a46b6e7a40b590545485f99f0d2a","placeholder":"​","style":"IPY_MODEL_efab1c955bf84eb691295d075973fde9","value":" 244/244 [00:00&lt;00:00, 3.09kB/s]"}},"b84e111f2b354895be518d2c2fe861ac":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b0c8635fce134b159ff0803b0d7737a8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b865426eba144607a0605ca32f183802":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c698040f165144ad878e04535e3df9b3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c33f3518c5cc4c2286f67ecc4377233f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d3b0a46b6e7a40b590545485f99f0d2a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"efab1c955bf84eb691295d075973fde9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7294103562ec4613b8f9d4d3dda94c26":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ec300372586345daacf736657c832d74","IPY_MODEL_8a0fe648df2e4e92a3c51f66b1b9be2c","IPY_MODEL_44cdb129953349ef81ef782ad075f002"],"layout":"IPY_MODEL_b217b545a5de4f2ebb065a2bc8a0a26c"}},"ec300372586345daacf736657c832d74":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_38d3081e402d4ec69320e18a74b4cdba","placeholder":"​","style":"IPY_MODEL_9c31bf54d84b4211b7073d536961b676","value":"config.json: 100%"}},"8a0fe648df2e4e92a3c51f66b1b9be2c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f322a0e0e571419992e3feeca1707b0a","max":872,"min":0,"orientation":"horizontal","style":"IPY_MODEL_be74537caa8641e7b7f331a55f094e04","value":872}},"44cdb129953349ef81ef782ad075f002":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4e1a1513c51d42ffbd4d145ab9143ed2","placeholder":"​","style":"IPY_MODEL_a301bd841894477985c6a370673a5605","value":" 872/872 [00:00&lt;00:00, 29.3kB/s]"}},"b217b545a5de4f2ebb065a2bc8a0a26c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"38d3081e402d4ec69320e18a74b4cdba":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9c31bf54d84b4211b7073d536961b676":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f322a0e0e571419992e3feeca1707b0a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"be74537caa8641e7b7f331a55f094e04":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4e1a1513c51d42ffbd4d145ab9143ed2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a301bd841894477985c6a370673a5605":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d07852437f5543bf97e5bbb5515d0370":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_707b7505b48942109fa3bf7d4bfab86d","IPY_MODEL_b431f21827074bce98e6c9c56606374d","IPY_MODEL_58839ff78c5641b09cfed6eebb687828"],"layout":"IPY_MODEL_1911d5d08d7a4c9ea6ef76e509daa524"}},"707b7505b48942109fa3bf7d4bfab86d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e6befaaeac6e4c7da14f4ebe702f7da9","placeholder":"​","style":"IPY_MODEL_3de058501bbe4e4ead92117e6a028fc3","value":"pytorch_model.bin: 100%"}},"b431f21827074bce98e6c9c56606374d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8841a0622b4244b3a27bc3f5320d4ea2","max":667409889,"min":0,"orientation":"horizontal","style":"IPY_MODEL_28f8681957154148a26105a7e92ec706","value":667409889}},"58839ff78c5641b09cfed6eebb687828":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e1f17b0c06e04568aa987885ef935b56","placeholder":"​","style":"IPY_MODEL_54021041ef4f4cbeab18263609019225","value":" 667M/667M [00:43&lt;00:00, 20.9MB/s]"}},"1911d5d08d7a4c9ea6ef76e509daa524":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e6befaaeac6e4c7da14f4ebe702f7da9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3de058501bbe4e4ead92117e6a028fc3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8841a0622b4244b3a27bc3f5320d4ea2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"28f8681957154148a26105a7e92ec706":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e1f17b0c06e04568aa987885ef935b56":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"54021041ef4f4cbeab18263609019225":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bf7302c9a25c432da1b9937a5211581e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_112a90ece83840658ab57e23a33d3178","IPY_MODEL_22ca77bc6252488381194e634bd7ad01","IPY_MODEL_6ff036256c5c4462beb0d01ac8137725"],"layout":"IPY_MODEL_fbe8a4b73c4a490d9444370731a8a44a"}},"112a90ece83840658ab57e23a33d3178":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c0ed08a1c3b146b091323ed1b87bbc8b","placeholder":"​","style":"IPY_MODEL_fe6f474f8f2641039af62f0d34af186e","value":"tokenizer_config.json: 100%"}},"22ca77bc6252488381194e634bd7ad01":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_fe76ff5bd4dd48439cf307d1891d0c11","max":790,"min":0,"orientation":"horizontal","style":"IPY_MODEL_633eb56c2b08466d825276eb04785fd5","value":790}},"6ff036256c5c4462beb0d01ac8137725":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2ffcb5b2cb3249dab48cb20c14ab3862","placeholder":"​","style":"IPY_MODEL_82463fd0e79c4c158d3dec72b3373ab7","value":" 790/790 [00:00&lt;00:00, 42.4kB/s]"}},"fbe8a4b73c4a490d9444370731a8a44a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c0ed08a1c3b146b091323ed1b87bbc8b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fe6f474f8f2641039af62f0d34af186e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fe76ff5bd4dd48439cf307d1891d0c11":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"633eb56c2b08466d825276eb04785fd5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2ffcb5b2cb3249dab48cb20c14ab3862":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"82463fd0e79c4c158d3dec72b3373ab7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"22af8692326a47c190727f6f2a61deb2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_76afa799c9bf497f8b63314e198c9902","IPY_MODEL_233a1ab3bd344b249a631b30b3e7692d","IPY_MODEL_047bd53bbc9f4f0c85d29bc5f2462c72"],"layout":"IPY_MODEL_659679c085ba4c0da055c43313d3d34d"}},"76afa799c9bf497f8b63314e198c9902":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_622cc4ff64664c8cb6361d8357db49ed","placeholder":"​","style":"IPY_MODEL_5ba4288315574269a6e4ac710c3b4ae8","value":"vocab.json: 100%"}},"233a1ab3bd344b249a631b30b3e7692d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7fff864bb1094e98ace0851088cbfabf","max":1274260,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f39f5dff34bb46f18554788501b50f08","value":1274260}},"047bd53bbc9f4f0c85d29bc5f2462c72":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_67b4f78c51b14f32b013fb1d0fb28ffa","placeholder":"​","style":"IPY_MODEL_3724d90edf36435b97f5043c8291c5a2","value":" 1.27M/1.27M [00:00&lt;00:00, 3.15MB/s]"}},"659679c085ba4c0da055c43313d3d34d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"622cc4ff64664c8cb6361d8357db49ed":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5ba4288315574269a6e4ac710c3b4ae8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7fff864bb1094e98ace0851088cbfabf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f39f5dff34bb46f18554788501b50f08":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"67b4f78c51b14f32b013fb1d0fb28ffa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3724d90edf36435b97f5043c8291c5a2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b0242fa532534059aad84fe362718bf5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_40896fbead00428eb859ffe2225ed629","IPY_MODEL_37a1f35dc98f45b0b7c5faa01e720700","IPY_MODEL_e5a0a716e89d420984fb636aa3b4f990"],"layout":"IPY_MODEL_4a7af1926780442da2aa8e64ff49ebb3"}},"40896fbead00428eb859ffe2225ed629":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a23299ee6d9043ef8b64725185a2e25e","placeholder":"​","style":"IPY_MODEL_7b4b7078766b45739e9f4c6de4bc956a","value":"merges.txt: 100%"}},"37a1f35dc98f45b0b7c5faa01e720700":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_292d94cc6dc4472c9a74d2adf93b8e1b","max":925250,"min":0,"orientation":"horizontal","style":"IPY_MODEL_00cf53a76f41403caab7d0aa0786afa8","value":925250}},"e5a0a716e89d420984fb636aa3b4f990":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_17cc9b07a77e4a208efe1f8328cd76cf","placeholder":"​","style":"IPY_MODEL_f1c3da6327684a01b16f3f59c2d85e0f","value":" 925k/925k [00:00&lt;00:00, 9.12MB/s]"}},"4a7af1926780442da2aa8e64ff49ebb3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a23299ee6d9043ef8b64725185a2e25e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7b4b7078766b45739e9f4c6de4bc956a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"292d94cc6dc4472c9a74d2adf93b8e1b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"00cf53a76f41403caab7d0aa0786afa8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"17cc9b07a77e4a208efe1f8328cd76cf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f1c3da6327684a01b16f3f59c2d85e0f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"df1c0c18aef648ce93e27fdb6d8ce44f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fdd8eb5b348c484182f83d0908f92673","IPY_MODEL_0e7d1fa105eb4ecd979b7cbd9e4bc378","IPY_MODEL_40efd0b097cf400aac321a3cdb3c7c02"],"layout":"IPY_MODEL_9ff44bb3838b4d3db31dbdd853e7a321"}},"fdd8eb5b348c484182f83d0908f92673":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_17fa1c5d971a4d4dad74b4ff6afcd769","placeholder":"​","style":"IPY_MODEL_a6fdd4bfc2414d53b639e2dadae7ee3b","value":"tokenizer.json: 100%"}},"0e7d1fa105eb4ecd979b7cbd9e4bc378":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6b70bda8384645f1b407aae4d4ff7452","max":3068334,"min":0,"orientation":"horizontal","style":"IPY_MODEL_820c65dc02dc43eb80a06b65d16acf56","value":3068334}},"40efd0b097cf400aac321a3cdb3c7c02":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c8dc933c5d7b4fb4a327f0daf6ccf7cc","placeholder":"​","style":"IPY_MODEL_66ed40b8379f4f5a84b09a5291b36ce9","value":" 3.07M/3.07M [00:00&lt;00:00, 7.77MB/s]"}},"9ff44bb3838b4d3db31dbdd853e7a321":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"17fa1c5d971a4d4dad74b4ff6afcd769":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a6fdd4bfc2414d53b639e2dadae7ee3b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6b70bda8384645f1b407aae4d4ff7452":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"820c65dc02dc43eb80a06b65d16acf56":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c8dc933c5d7b4fb4a327f0daf6ccf7cc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"66ed40b8379f4f5a84b09a5291b36ce9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"aec93cb16b84497892c9feb35198d000":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_dee02e5b80ee4cb0bf6678e0cd6af567","IPY_MODEL_77e2ec64d1d04e98b2ae8a65c3e42aee","IPY_MODEL_07d86920917f466da3c6c55c11b9af26"],"layout":"IPY_MODEL_94cb896d0d844b809239426863584f29"}},"dee02e5b80ee4cb0bf6678e0cd6af567":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_83742ecaf1564233b70509695e9c3320","placeholder":"​","style":"IPY_MODEL_5544b2fae7274cf8bb0c947965a23a9e","value":"special_tokens_map.json: 100%"}},"77e2ec64d1d04e98b2ae8a65c3e42aee":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_03dc7c4dd0254caa84215096a1a786dc","max":96,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1a5e73cc1c2e49f786eecf360284d605","value":96}},"07d86920917f466da3c6c55c11b9af26":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_59e936ced9544af3b764e94bc2cf7120","placeholder":"​","style":"IPY_MODEL_bb9cdaa4040848e58f62d8624fe0b2d5","value":" 96.0/96.0 [00:00&lt;00:00, 1.28kB/s]"}},"94cb896d0d844b809239426863584f29":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"83742ecaf1564233b70509695e9c3320":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5544b2fae7274cf8bb0c947965a23a9e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"03dc7c4dd0254caa84215096a1a786dc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1a5e73cc1c2e49f786eecf360284d605":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"59e936ced9544af3b764e94bc2cf7120":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bb9cdaa4040848e58f62d8624fe0b2d5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["# **1. 문장 임베딩**\n","* 2017년 이전의 임베딩 기법들은 대부분 단어 수준의 모델\n","* 단어 수준 임베딩 기법은 자연어의 특성인 모호성, 동음이의어를 구분하기 어렵다는 한계가 있음\n","* 2017년 이후에는 ELMo(Embeddings from Language Models)와 같은 모델이 발표되고 트랜스포머와 같은 언어 모델에서 문장 수준의 언어 모델링을 고려하면서 한계점들이 해결됨"],"metadata":{"id":"QHLNeWXI8q6N"}},{"cell_type":"markdown","source":["# **2. seq2seq 배경**\n","* seq2seq 모델이 등장하기 전에 DNN(Deep Neural Network) 모델은 사물인식, 음성인식 등에서 꾸준히 성과를 내고 있었음(예: CNN, RNN, LSTM, GRU ...)\n","* 모델 입/출력의 크기가 고정된다는 한계점이 존재했기 때문에 자연어처리와 같은 가변적인 길이의 입/출력을 처리하는 문제들을 제대로 해결할 수 없었음\n","* RNN은 seqseq가 등장하기 전에 입/출력을 시퀀스 단위로 처리할 수 있는 모델이었음"],"metadata":{"id":"PC5xDilB9fwb"}},{"cell_type":"markdown","source":["### 2-1. seq2seq(Sequence To Sequence)란?\n","* 2014년 구글에서 [논문](https://arxiv.org/abs/1409.3215)으로 제안한 모델\n","    * 논문: https://arxiv.org/abs/1409.3215\n","* LSTM 또는 GRU기반의 구조를 가지고 고정된 길이의 단어 시퀀스를 입력받아 입력 시퀀스에 알맞은 길이의 시퀀스를 출력해주는 언어 모델\n","* 2개의 LSTM을 각각 Encoder와 Decoder로 사용해 가변적인 길이의 입출력을 처리하고자 했음\n","* 기계 번역 작업에서 큰 성능 향상을 가져왔고 특히 긴 문장을 처리하는데 강점이 있음\n"],"metadata":{"id":"sjG5IW5u-1HH"}},{"cell_type":"markdown","source":["### 2-2. 인코더\n","* 입력 문장을 컨텍스트 벡터에 압축하는 역할\n","* 인코더의 LSTM은 입력 문장을 단어 순서대로 처리하여 고정된 크기의 컨텍스트 벡터를 반환\n","* 컨텍스트 벡터는 인코더의 마지막 스텝에서 출력된 hidden state와 같음\n","* 컨텍스트 벡터는 입력 문장의 정보를 함축하는 벡터이므로 해당 벡터를 입력 문장에 대한 수준의 벡터로 활용할 수 있음\n","\n","```\n","    class Encoder(nn.Module):\n","        def __init__(self, input_size, hidden_size):\n","            super(Encoder, self).__init__()\n","            self.input_size = input_size\n","            self.hidden_size = hidden_size\n","            self.embedding = nn.Embedding(input_size, hidden_size)\n","            self.gru = nn.GRU(input_size, hidden_size)\n","        \n","        def forward(self, input):\n","            embedded = self.embedding(input).view(1, 1, -1)\n","            output, hidden = self.gru(embedded)\n","            return output, hidden\n","        \n","    class Decoder(nn.Module):\n","        def __init__(self, hidden_size, output_size):\n","            super(Decoder, self).__init__()\n","            self.hidden_size = hideen_size\n","            self.output_size = output_size\n","            self.embedding = nn.Embedding(output_size, hidden_size)\n","            self.gru = nn.GRU(embedding, hidden_size)\n","            self.out = nn.Linear(hidden_size, output_size)\n","        \n","        def forward(self, input, hidden):\n","            output = self.embedding(input).view(1, 1, -1)\n","            output = F.relu(output)\n","            output, hidden = self.gru(output, hidden)\n","            output = self.out(output[0])\n","            return output, hidden\n","```"],"metadata":{"id":"jLuIqL7-AHxU"}},{"cell_type":"markdown","source":["### 2-3. 디코더\n","* 입력 문장의 정보가 압축된 컨텍스트 벡터를 사용하여 출력 문장을 디코딩하는 역할\n","* 컨텐스트 벡터와 문장의 시작을 뜻하는 토큰을 입력으로 받아서 문장의 끝을 뜻하는 토큰이 나올 때까지 문장을 생성\n","* LSTM의 첫 셀에서는 토큰과 컨텍스트 벡터를 입력받아서 그 다음에 등장할 확률이 가장 높은 단어를 예측하고 다음 스텝에서 예측한 단어를 입력으로 받아 그 다음에 등장할 확률이 가장 높은 단어를 예측하는 형태로 계속 진행\n","* 위 과정을 재귀적으로 반복하다가 다음에 등장할 확률이 가장 높은 단어로 토큰이 나오면 생성을 종료"],"metadata":{"id":"aDrcHztXDwBu"}},{"cell_type":"markdown","source":["### 2-4. 학습과정과 한계점\n","* 모델 학습 과정에서는 이전 셀에서 예측한 단어를 다음 셀의 입력으로 넣어주는 대신 실제 정답 단어를 다음 셀의 입력으로 넣기도 함(교사 강요(Teacher Forcing))\n","    * 위 방법으로 학습되지 않으면 이전 셀에서의 오류가 다음 셀로 계속 전파될 것이기 때문에 학습이 제대로 되지 않고 오래 걸릴 수 있음\n","* 가변적인 길이의 입/출력을 처리하는 데 효과적인 모델 구조이며, 실제로 기계 번역 작업에서 성능 향상을 거뒀으나 여전히 한계를 가짐\n","    * 인코더가 출력하는 벡터 사이즈가 고정되어 있기 때문에 입력으로 들어오는 단어의 수가 매우 많아지면 성능이 떨어짐\n","    * RNN 구조의 모델에서는 hidden state를 통해 이전 셀의 정보를 다음 셀로 계속 전달하게 되는데 문장의 길이가 길어지면 초기 셀에서 전달됐던 정보들이 점차 흐려짐(LSTM, GRU 같은 모델들이 제안되긴 했으나 여전히 이전 정보를 계속 압축하는데 한계가 있음)"],"metadata":{"id":"0fF1WUyrENFP"}},{"cell_type":"code","source":["import os\n","import re\n","import shutil\n","import zipfile\n","import requests\n","import numpy as np\n","import pandas as pd\n","import unicodedata\n","import urllib3\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import numpy as np\n","import torch.nn.functional as F\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from torch.utils.data import DataLoader, TensorDataset"],"metadata":{"id":"jdLz38wiOA9-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# fra-eng.zip  다운로드하고 압축해제\n","# fra-eng.zip 프랑스어-영어 병렬 코퍼스인  파일\n","# ( 19만개의 병렬 문장 샘플  / 예) Watch me. Regardez-moi !)\n","headers = {\n","    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n","}\n","\n","def download_zip(url, output_path):\n","    response = requests.get(url, headers=headers, stream=True)\n","    if response.status_code == 200:\n","        with open(output_path, 'wb') as f:\n","            for chunk in response.iter_content(chunk_size=8192):\n","                f.write(chunk)\n","        print(f\"ZIP file downloaded to {output_path}\")\n","    else:\n","        print(f\"Failed to download. HTTP Response Code: {response.status_code}\")\n","\n","url = \"http://www.manythings.org/anki/fra-eng.zip\"\n","output_path = \"fra-eng.zip\"\n","download_zip(url, output_path)\n","\n","path = os.getcwd()\n","zipfilename = os.path.join(path, output_path)\n","\n","with zipfile.ZipFile(zipfilename, 'r') as zip_ref:\n","    zip_ref.extractall(path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KzThnSMxAk3M","executionInfo":{"status":"ok","timestamp":1720069526456,"user_tz":-540,"elapsed":1685,"user":{"displayName":"Jeongwon Ryu","userId":"18132725682754503053"}},"outputId":"0ca2a03f-e6df-47b4-db20-e5f9ae640300"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["ZIP file downloaded to fra-eng.zip\n"]}]},{"cell_type":"code","source":["# 약 19만개의 데이터 중 33,000개의 샘플만을 사용\n","num_samples = 33000"],"metadata":{"id":"-8JjT_OgA3gJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Seq2Seq 데이터 전처리"],"metadata":{"id":"9FfWaWfiWifb"}},{"cell_type":"code","source":["# 전처리 함수 :  구두점 제거 / 단어 구분\n","def to_ascii(s):\n","  # 프랑스어 악센트(accent) 삭제\n","  # 예시 : 'déjà diné' -> deja dine\n","  return ''.join(c for c in unicodedata.normalize('NFD', s)\n","                   if unicodedata.category(c) != 'Mn')\n","\n","def preprocess_sentence(sent):\n","  # 악센트 제거 함수 호출\n","  sent = to_ascii(sent.lower())\n","\n","  # 단어와 구두점 사이에 공백 추가.\n","  # ex) \"I am a student.\" => \"I am a student .\"\n","  sent = re.sub(r\"([?.!,¿])\", r\" \\1\", sent)\n","\n","  # (a-z, A-Z, \".\", \"?\", \"!\", \",\") 이들을 제외하고는 전부 공백으로 변환.\n","  sent = re.sub(r\"[^a-zA-Z!.?]+\", r\" \", sent)\n","\n","  # 다수 개의 공백을 하나의 공백으로 치환\n","  sent = re.sub(r\"\\s+\", \" \", sent)\n","  return sent"],"metadata":{"id":"Lye6ddozA9a3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 전처리 테스트\n","# 임의의 문장 입력테스트\n","en_sent = u\"Have you had dinner?\"\n","fr_sent = u\"Avez-vous déjà diné?\"\n","\n","print('전처리 전 영어 문장 :', en_sent)\n","print('전처리 후 영어 문장 :',preprocess_sentence(en_sent))\n","print('전처리 전 프랑스어 문장 :', fr_sent)\n","print('전처리 후 프랑스어 문장 :', preprocess_sentence(fr_sent))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"flmimZdMBJQA","executionInfo":{"status":"ok","timestamp":1720069526456,"user_tz":-540,"elapsed":2,"user":{"displayName":"Jeongwon Ryu","userId":"18132725682754503053"}},"outputId":"619a6314-425e-45b2-c44e-6855c5858e1b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["전처리 전 영어 문장 : Have you had dinner?\n","전처리 후 영어 문장 : have you had dinner ?\n","전처리 전 프랑스어 문장 : Avez-vous déjà diné?\n","전처리 후 프랑스어 문장 : avez vous deja dine ?\n"]}]},{"cell_type":"code","source":["# 전체 데이터에서 33,000개의 샘플에 대해서 전처리를 수행\n","# 훈련 과정에서 교사 강요(Teacher Forcing)을 사용\n","# > 훈련 시 사용할 디코더의 입력 시퀀스와 실제값. 즉, 레이블에 해당되는 출력 시퀀스를 따로 분리하여 저장\n","# > 입력 시퀀스에는 시작을 의미하는 토큰인 <sos>를 추가,\n","#   출력 시퀀스에는 종료를 의미하는 토큰인 <eos>를 추가\n","def load_preprocessed_data():\n","  encoder_input, decoder_input, decoder_target = [], [], []\n","\n","  with open(\"fra.txt\", \"r\") as lines:\n","    for i, line in enumerate(lines):\n","      # source 데이터와 target 데이터 분리\n","      src_line, tar_line, _ = line.strip().split('\\t')\n","\n","      # source 데이터 전처리\n","      src_line = [w for w in preprocess_sentence(src_line).split()]\n","\n","      # target 데이터 전처리\n","      # 각 문장의 시작과 끝을 나타내는 특수 토큰들을 추가하여 데이터를 준비하는 과정을 수행\n","      # 입력 문장이 \"source 데이터\"가 되고, 번역하려는 문장이 \"target 데이터\"\n","      # \"target 데이터\"는 모델이 출력해야 할 정답 혹은 목표 문장\n","      tar_line = preprocess_sentence(tar_line)\n","      tar_line_in = [w for w in (\"<sos> \" + tar_line).split()]\n","      tar_line_out = [w for w in (tar_line + \" <eos>\").split()]\n","\n","      encoder_input.append(src_line)\n","      decoder_input.append(tar_line_in)\n","      decoder_target.append(tar_line_out)\n","\n","      if i == num_samples - 1:\n","        break\n","\n","  return encoder_input, decoder_input, decoder_target"],"metadata":{"id":"dWFJ7NwNBLxx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sents_en_in, sents_fra_in, sents_fra_out = load_preprocessed_data()\n","print('인코더의 입력 :',sents_en_in[:5])\n","print('디코더의 입력 :',sents_fra_in[:5])\n","print('디코더의 레이블 :',sents_fra_out[:5])\n","# 디코더의 입력에 해당하는 데이터인 sents_fra_in이 왜 필요?\n","# 그 이유는 이전 시점의 디코더 셀의 예측이 틀렸는데 이를 현재 시점의 디코더 셀의 입력으로 사용하면\n","# 현재 시점의 디코더 셀의 예측도 잘못될 가능성이 높고\n","# 이는 연쇄 작용으로 디코더 전체의 예측을 어렵게 합니다.\n","# 이런 상황이 반복되면 훈련 시간이 느려집니다.\n","# 만약 이 상황을 원하지 않는다면\n","# 이전 시점의 디코더 셀의 예측값 대신 실제값을 현재 시점의 디코더 셀의 입력으로 사용하는 방법을 사용할 수 있습니다.\n","# 이와 같이 RNN의 모든 시점에 대해서 이전 시점의 예측값 대신 실제값을 입력으로 주는 방법을 교사 강요라고 합니다."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-ayvumfABk7v","executionInfo":{"status":"ok","timestamp":1720069528833,"user_tz":-540,"elapsed":2378,"user":{"displayName":"Jeongwon Ryu","userId":"18132725682754503053"}},"outputId":"34cd5fcb-5849-4f94-e5ae-d4f105224426"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["인코더의 입력 : [['go', '.'], ['go', '.'], ['go', '.'], ['go', '.'], ['hi', '.']]\n","디코더의 입력 : [['<sos>', 'va', '!'], ['<sos>', 'marche', '.'], ['<sos>', 'en', 'route', '!'], ['<sos>', 'bouge', '!'], ['<sos>', 'salut', '!']]\n","디코더의 레이블 : [['va', '!', '<eos>'], ['marche', '.', '<eos>'], ['en', 'route', '!', '<eos>'], ['bouge', '!', '<eos>'], ['salut', '!', '<eos>']]\n"]}]},{"cell_type":"code","source":["# 케라스 토크나이저를 통해 단어 집합을 생성,\n","# 정수 인코딩을 진행 후 이어서 패딩을 진행\n","tokenizer_en = Tokenizer(filters=\"\", lower=False)\n","tokenizer_en.fit_on_texts(sents_en_in)\n","encoder_input = tokenizer_en.texts_to_sequences(sents_en_in)\n","encoder_input = pad_sequences(encoder_input, padding=\"post\")\n","\n","tokenizer_fra = Tokenizer(filters=\"\", lower=False)\n","tokenizer_fra.fit_on_texts(sents_fra_in)\n","tokenizer_fra.fit_on_texts(sents_fra_out)\n","\n","decoder_input = tokenizer_fra.texts_to_sequences(sents_fra_in)\n","decoder_input = pad_sequences(decoder_input, padding=\"post\")\n","\n","decoder_target = tokenizer_fra.texts_to_sequences(sents_fra_out)\n","decoder_target = pad_sequences(decoder_target, padding=\"post\")"],"metadata":{"id":"e2r6QUOSBnIW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('인코더의 입력의 크기(shape) :',encoder_input.shape)\n","print('디코더의 입력의 크기(shape) :',decoder_input.shape)\n","print('디코더의 레이블의 크기(shape) :',decoder_target.shape)\n","# 샘플은 총 33,000개 존재하며 영어 문장의 길이는 8, 프랑스어 문장의 길이는 16"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8wEjVgDPCIZ5","executionInfo":{"status":"ok","timestamp":1720069531100,"user_tz":-540,"elapsed":10,"user":{"displayName":"Jeongwon Ryu","userId":"18132725682754503053"}},"outputId":"1fc7a430-d535-46f4-a924-06358c91fd07"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["인코더의 입력의 크기(shape) : (33000, 7)\n","디코더의 입력의 크기(shape) : (33000, 16)\n","디코더의 레이블의 크기(shape) : (33000, 16)\n"]}]},{"cell_type":"code","source":["# 단어 집합의 크기를 정의\n","src_vocab_size = len(tokenizer_en.word_index) + 1\n","tar_vocab_size = len(tokenizer_fra.word_index) + 1\n","print(\"영어 단어 집합의 크기 : {:d}, 프랑스어 단어 집합의 크기 : {:d}\".format(src_vocab_size, tar_vocab_size))\n","# 단어 집합의 크기는 각각 4485개와 7878개\n","# 단어로부터 정수를 얻는 딕셔너리와 정수로부터 단어를 얻는 딕셔너리를 각각 만들어줍니다.\n","# 이들은 훈련을 마치고 예측값과 실제값을 비교하는 단계에서 사용"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TjQQ_eVDCjVq","executionInfo":{"status":"ok","timestamp":1720069531100,"user_tz":-540,"elapsed":9,"user":{"displayName":"Jeongwon Ryu","userId":"18132725682754503053"}},"outputId":"8e3c247f-729f-4c10-b8bf-68b43d3306b7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["영어 단어 집합의 크기 : 4485, 프랑스어 단어 집합의 크기 : 7878\n"]}]},{"cell_type":"code","source":["# 단어로부터 정수를 얻는 딕셔너리와 정수로부터 단어를 얻는 딕셔너리를 각각 만들어주기\n","# 훈련을 마치고 예측값과 실제값을 비교하는 단계에서 사용\n","src_to_index = tokenizer_en.word_index\n","index_to_src = tokenizer_en.index_word\n","tar_to_index = tokenizer_fra.word_index\n","index_to_tar = tokenizer_fra.index_word"],"metadata":{"id":"Vu07A9NSClm0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 테스트 데이터를 분리하기 전 데이터 섞기\n","# 순서가 섞인 정수 시퀀스 리스트를 만들기\n","indices = np.arange(encoder_input.shape[0])\n","np.random.shuffle(indices)\n","print('랜덤 시퀀스 :',indices)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ueqG2AmeCwrA","executionInfo":{"status":"ok","timestamp":1720069531100,"user_tz":-540,"elapsed":8,"user":{"displayName":"Jeongwon Ryu","userId":"18132725682754503053"}},"outputId":"76a8b916-bb34-40eb-80a2-2254c3ae1633"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["랜덤 시퀀스 : [ 4901 26325   566 ...  9237 18791 29585]\n"]}]},{"cell_type":"code","source":["# 데이터셋의 순서로 지정해주면 샘플들이 기존 순서와 다른 순서로 섞이게 됨\n","encoder_input = encoder_input[indices]\n","decoder_input = decoder_input[indices]\n","decoder_target = decoder_target[indices]"],"metadata":{"id":"zfjyE3hpC0W0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 임의로 30,997번째 샘플을 출력\n","encoder_input[30997]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1J6SiuJSC7Tz","executionInfo":{"status":"ok","timestamp":1720069531101,"user_tz":-540,"elapsed":8,"user":{"displayName":"Jeongwon Ryu","userId":"18132725682754503053"}},"outputId":"e5897def-062c-4322-8b75-cd0219f9e9a4"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([  3,  69,  56, 904,   1,   0,   0], dtype=int32)"]},"metadata":{},"execution_count":27}]},{"cell_type":"code","source":["# decoder_input과 decoder_target은 데이터의 구조상으로\n","# 앞에 붙은 <sos> 토큰과 뒤에 붙은 <eos>을 제외하면 동일한 정수 시퀀스를 가져야함\n","# 18, 5, 16, 173, 1이라는 동일 시퀀스를 확인\n","decoder_input[30997]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WrNsUgjiDAhJ","executionInfo":{"status":"ok","timestamp":1720069531101,"user_tz":-540,"elapsed":7,"user":{"displayName":"Jeongwon Ryu","userId":"18132725682754503053"}},"outputId":"81a89fca-8ff2-40d8-a318-580fc8c9d3da"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([  2,  17,  49,  15, 996,   1,   0,   0,   0,   0,   0,   0,   0,\n","         0,   0,   0], dtype=int32)"]},"metadata":{},"execution_count":28}]},{"cell_type":"code","source":["decoder_target[30997]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_p5Erlt3DDbD","executionInfo":{"status":"ok","timestamp":1720069531101,"user_tz":-540,"elapsed":6,"user":{"displayName":"Jeongwon Ryu","userId":"18132725682754503053"}},"outputId":"6fb237f4-5308-4ebc-b635-a4fa164a5b7c"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([ 17,  49,  15, 996,   1,   3,   0,   0,   0,   0,   0,   0,   0,\n","         0,   0,   0], dtype=int32)"]},"metadata":{},"execution_count":29}]},{"cell_type":"code","source":["# 훈련 데이터의 10%를 테스트 데이터로 분리\n","n_of_val = int(33000*0.1)\n","print('검증 데이터의 개수 :',n_of_val)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nLQNC6YODF-5","executionInfo":{"status":"ok","timestamp":1720069531101,"user_tz":-540,"elapsed":5,"user":{"displayName":"Jeongwon Ryu","userId":"18132725682754503053"}},"outputId":"adaf320a-4ff0-47f5-9ffd-3d3f325753ba"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["검증 데이터의 개수 : 3300\n"]}]},{"cell_type":"code","source":["# 33,000개의 10%에 해당되는 3,300개의 데이터를 테스트 데이터로 사용\n","# [-n_of_val:]는 전체 데이터에서 마지막 n_of_val 개의 데이터만 선택\n","encoder_input_train = encoder_input[:-n_of_val]\n","decoder_input_train = decoder_input[:-n_of_val]\n","decoder_target_train = decoder_target[:-n_of_val]\n","\n","encoder_input_test = encoder_input[-n_of_val:]\n","decoder_input_test = decoder_input[-n_of_val:]\n","decoder_target_test = decoder_target[-n_of_val:]"],"metadata":{"id":"r-ONlvsTDLg2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 훈련 데이터와 테스트 데이터의 크기(shape)를 출력\n","print('훈련 source 데이터의 크기 :',encoder_input_train.shape)\n","print('훈련 target 데이터의 크기 :',decoder_input_train.shape)\n","print('훈련 target 레이블의 크기 :',decoder_target_train.shape)\n","print('테스트 source 데이터의 크기 :',encoder_input_test.shape)\n","print('테스트 target 데이터의 크기 :',decoder_input_test.shape)\n","print('테스트 target 레이블의 크기 :',decoder_target_test.shape)\n","# 훈련 데이터의 샘플은 29,700개, 테스트 데이터의 샘플은 3,300개가 존재"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z6Bwi1srDRW7","executionInfo":{"status":"ok","timestamp":1720069531101,"user_tz":-540,"elapsed":5,"user":{"displayName":"Jeongwon Ryu","userId":"18132725682754503053"}},"outputId":"ed184564-9926-455f-aa37-df79a39220cb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["훈련 source 데이터의 크기 : (29700, 7)\n","훈련 target 데이터의 크기 : (29700, 16)\n","훈련 target 레이블의 크기 : (29700, 16)\n","테스트 source 데이터의 크기 : (3300, 7)\n","테스트 target 데이터의 크기 : (3300, 16)\n","테스트 target 레이블의 크기 : (3300, 16)\n"]}]},{"cell_type":"markdown","source":["Seq2Seq 기계 번역기 만들기\n","* 참고: https://wikidocs.net/86900"],"metadata":{"id":"Wz3eQwvQWaY7"}},{"cell_type":"markdown","source":["### Seq2Seq(sequence-to-sequence) 모델을 구현"],"metadata":{"id":"juwNNZ8JX8DY"}},{"cell_type":"code","source":["# 임베딩 벡터의 차원과 LSTM의 은닉 상태의 크기를 64로 사용\n","embedding_dim = 64\n","hidden_units = 64"],"metadata":{"id":"K1Wfsbs2DTVt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Encoder(nn.Module):\n","    def __init__(self, input_size, hidden_size):\n","        super(Encoder, self).__init__()\n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","        self.embedding = nn.Embedding(input_size, hidden_size)\n","        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n","\n","    def forward(self, input):\n","        embedded = self.embedding(input)\n","        output, hidden = self.gru(embedded)\n","        return output, hidden\n","\n","class Decoder(nn.Module):\n","    def __init__(self, hidden_size, output_size):\n","        super(Decoder, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.output_size = output_size\n","        self.embedding = nn.Embedding(output_size, hidden_size)\n","        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n","        self.out = nn.Linear(hidden_size, output_size)\n","\n","    def forward(self, input, hidden):\n","        embedded = self.embedding(input)\n","        embedded = F.relu(embedded)\n","        output, hidden = self.gru(embedded, hidden)\n","        output = self.out(output)\n","        return output, hidden"],"metadata":{"id":"zTxef_djDasw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Seq2Seq(nn.Module):\n","    def __init__(self, encoder, decoder):\n","        super(Seq2Seq, self).__init__()\n","        self.encoder = encoder\n","        self.decoder = decoder\n","\n","    def forward(self, src, trg):\n","        encoder_outputs, hidden = self.encoder(src)\n","        decoder_outputs, _ = self.decoder(trg, hidden)\n","        return decoder_outputs"],"metadata":{"id":"v0HYBbvOItto"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["encoder = Encoder(src_vocab_size, hidden_units)\n","decoder = Decoder(hidden_units, tar_vocab_size)\n","model = Seq2Seq(encoder, decoder)"],"metadata":{"id":"39rMhqOUJE1T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["loss_func = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters())"],"metadata":{"id":"L4F2Xp-OOsUW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["encoder_input_train_tensor = torch.tensor(encoder_input_train, dtype=torch.long)\n","decoder_input_train_tensor = torch.tensor(decoder_input_train, dtype=torch.long)\n","decoder_target_train_tensor = torch.tensor(decoder_target_train, dtype=torch.long)\n","\n","encoder_input_test_tensor = torch.tensor(encoder_input_test, dtype=torch.long)\n","decoder_input_test_tensor = torch.tensor(decoder_input_test, dtype=torch.long)\n","decoder_target_test_tensor = torch.tensor(decoder_target_test, dtype=torch.long)"],"metadata":{"id":"PKVx8vLeUr8c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_dataset = TensorDataset(encoder_input_train_tensor, decoder_input_train_tensor, decoder_target_train_tensor)\n","test_dataset = TensorDataset(encoder_input_test_tensor, decoder_input_test_tensor, decoder_target_test_tensor)\n","\n","batch_size = 64\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"],"metadata":{"id":"cg2wCfdaYOvO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train_model(model, optimizer, loss_func, train_loader, epochs):\n","    model.train()\n","    for epoch in range(epochs):\n","        total_loss = 0\n","        for encoder_input_train_tensor, decoder_input_train_tensor, decoder_target_train_tensor in train_loader:\n","            optimizer.zero_grad()\n","            outputs = model(encoder_input_train_tensor, decoder_input_train_tensor)\n","            loss = loss_func(outputs.view(-1, outputs.shape[-1]), decoder_target_train_tensor.view(-1))\n","            loss.backward()\n","            optimizer.step()\n","            total_loss += loss.item()\n","        print(f'Epoch [{epoch+1}/{epochs}], Loss: {total_loss/len(train_loader):.4f}')\n","\n","# 학습 시작\n","train_model(model, optimizer, loss_func, train_loader, epochs=10)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nFRkBodyZBPL","executionInfo":{"status":"ok","timestamp":1720070258455,"user_tz":-540,"elapsed":724971,"user":{"displayName":"Jeongwon Ryu","userId":"18132725682754503053"}},"outputId":"ddb4b6a8-0055-43b5-b20a-3db7a8bac6a1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch [1/10], Loss: 2.1650\n","Epoch [2/10], Loss: 1.3618\n","Epoch [3/10], Loss: 1.1642\n","Epoch [4/10], Loss: 1.0505\n","Epoch [5/10], Loss: 0.9722\n","Epoch [6/10], Loss: 0.9117\n","Epoch [7/10], Loss: 0.8620\n","Epoch [8/10], Loss: 0.8203\n","Epoch [9/10], Loss: 0.7839\n","Epoch [10/10], Loss: 0.7526\n"]}]},{"cell_type":"code","source":["# 디코드 시퀀스 함수\n","def decode_sequence(input_seq):\n","    input_tensor = torch.tensor(input_seq, dtype=torch.long)\n","    with torch.no_grad():\n","        encoder_output, hidden = model.encoder(input_tensor)\n","\n","    target_seq = torch.tensor([[tar_to_index['<sos>']]], dtype=torch.long)\n","    decoded_sentence = ''\n","\n","    stop_condition = False\n","    while not stop_condition:\n","        with torch.no_grad():\n","            output, hidden = model.decoder(target_seq, hidden)\n","\n","        sampled_token_index = output.argmax(2).item()\n","        sampled_char = index_to_tar[sampled_token_index]\n","\n","        if sampled_char != '<eos>':\n","            decoded_sentence += ' ' + sampled_char\n","\n","        if sampled_char == '<eos>' or len(decoded_sentence.split()) > 100:\n","            stop_condition = True\n","\n","        target_seq = torch.tensor([[sampled_token_index]], dtype=torch.long)\n","\n","    return decoded_sentence\n","\n","# 원문의 정수 시퀀스를 텍스트 시퀀스로 변환\n","def seq_to_src(input_seq):\n","    sentence = ''\n","    for encoded_word in input_seq:\n","        if encoded_word != 0:\n","            sentence += index_to_src.get(encoded_word, '') + ' '\n","    return sentence.strip()\n","\n","# 번역문의 정수 시퀀스를 텍스트 시퀀스로 변환\n","def seq_to_tar(input_seq):\n","    sentence = ''\n","    for encoded_word in input_seq:\n","        if encoded_word != 0 and encoded_word != tar_to_index['<sos>'] and encoded_word != tar_to_index['<eos>']:\n","            sentence += index_to_tar.get(encoded_word, '') + ' '\n","    return sentence.strip()\n","\n","# 테스트\n","for seq_index in [0, 1]:\n","    input_seq = encoder_input_train[seq_index: seq_index + 1]\n","    decoded_sentence = decode_sequence(input_seq)\n","\n","    print(\"입력문장 :\", seq_to_src(encoder_input_train[seq_index]))\n","    print(\"정답문장 :\", seq_to_tar(decoder_input_train[seq_index]))\n","    print(\"번역문장 :\", decoded_sentence.strip())\n","    print(\"-\" * 50)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MbeR31jUfDS_","executionInfo":{"status":"ok","timestamp":1720070258456,"user_tz":-540,"elapsed":7,"user":{"displayName":"Jeongwon Ryu","userId":"18132725682754503053"}},"outputId":"8952bbfe-51c9-4741-8bb2-f5e0eb1ed253"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["입력문장 : do you smoke ?\n","정답문장 : tu fumes ?\n","번역문장 : est ce que tu as ?\n","--------------------------------------------------\n","입력문장 : her eyes darkened .\n","정답문장 : ses yeux s assombrirent .\n","번역문장 : donne moi la balle .\n","--------------------------------------------------\n"]}]},{"cell_type":"markdown","source":["# **3. 어텐션 메커니즘**\n","* seq2seq 모델의 한계를 해결하기 위해 제안한 논문에서 발표\n","* 어텐션이라는 단어가 쓰이지 않았지만 어텐션 개념을 제공한 [연구논문](https://arxiv.org/abs/1409.0473)\n","    * https://arxiv.org/abs/1409.0473\n","* 어텐션 단어를 사용한 모델에 대한 [논문](https://arxiv.org/abs/1508.04025)\n","    * https://arxiv.org/abs/1508.04025\n","* 입력 시퀀스가 길어지면 출력 시퀀스의 정확도가 떨어지는 것을 보정해주기 위해 등장한 기법\n","* 어텐션의 아이디어는 디코더에서 출력 단어를 예측하는 매 시점(time step)마다 인코더에서의 전체 입력 문장을 다시 한 번 참고하는 것\n","* 단, 전체 입력 문장을 전부 다 동일한 비율로 참고하는 것이 아니라 해당 시점에서 예측해야 할 단어와 연관이 있는 입력 단어 부분을 좀 더 집중해서 보게 함\n","* 참고사이트: https://wikidocs.net/22893\n"],"metadata":{"id":"kZPTcr1XFroJ"}},{"cell_type":"markdown","source":["### 3-1. 어텐션 함수\n","* 어텐션을 함수로 표현 -> Attention(Q,K,V) = Attention Value\n","* 어텐션 함수는 주어진 쿼리에 대하여 모든 키와의 유사도를 각각 계산\n","* 계산된 유사도를 키와 맵핑되어 있는 각각의 값에 반영한 뒤 유사도와 반영된 값을 모두 더해서 반환(어텐션 값)\n","    * Q(Query): t시점의 디코더 셀에서의 은닉 상태\n","    * K(Keys): 모든 시점의 인코더 셀의 은닉 상태들\n","    * V(Values): 모든 시점의 인코더 셀의 값"],"metadata":{"id":"rTe7XJ-rLEmX"}},{"cell_type":"markdown","source":["### 3-2. 어텐션과 Seq2Seq\n","* 어텐션 메커니즘은 Seq2Seq 모델이 가지는 한계를 해결하기 위해 제안 되었기 때문에 논문에서는 Seq2Seq 모델에 어텐션 메커니즘을 적용한 모델을 제안"],"metadata":{"id":"b8RwkXPgZRzR"}},{"cell_type":"markdown","source":["### 3-2. 어텐션의 작동 원리\n","* 시점의 예측하고자 하는 단어를 위해 입력 단어들의 정보를 다시 참고\n","* 어텐션 스코어를 구하는데 사용하는 수식은 다양하게 있으나 가장 간단한 dot product를 사용하는 것이 일반적\n","* 단어들의 정보를 참고하여 나온 확률 중 가장 큰 값을 예측하고자 하는 단어를 위해 사용함"],"metadata":{"id":"2rXBSTo9NCLu"}},{"cell_type":"markdown","source":["# **4. ELMo(Embeddings from Language Model)**\n","* 2018년 [논문](https://arxiv.org/abs/1802.05365)에서 제안된 새로운 워드 임베딩 방법론\n","    * https://arxiv.org/abs/1802.05365\n","* 엘모의 가장 큰 특징은 사전 훈련된 언어 모델을 사용한다는 것\n","* 기존 워드 임베딩은 주변 문맥 정보를 활용하여 단어를 벡터로 표현하는 방법을 사용(같은 표기 단어를 문맥에 따라 다르게 임베딩 할 수 없는 한계가 있었음)\n","* biLM이라는 구조를 사용(양방향의 언어 모델링을 통해 문맥적인 표현을 반영하여 해당 입력 문장의 확률을 예측)\n","* 대량의 자연어 코퍼스를 미리 학습하여 코퍼스 안에 포함된 일반화된 언어 특성들을 모델의 파라미터 안에 함축하여 사용하는 방법\n","* 참고사이트: https://wikidocs.net/33930"],"metadata":{"id":"KW7tO94rNmJd"}},{"cell_type":"markdown","source":["### 허깅페이스(Hugging Face)\n","* [허깅페이스](https://huggingface.co/)\n","    * 사이트:  https://huggingface.co/\n","* 인공지능 자연어 처리 기술을 중심으로 한 오픈소스 커뮤니티와 소프트웨어 플랫폼을 제공하는 사이트(회사)\n","* 특히 트랜스포머 모델들을 쉽게 사용할 수 있도록 하는 라이브러리(Transformers)로 유명\n","* 플랫폼과 라이브러리 등은 개발자와 AI 기업들에게 쉽게 학습시키고 배포할 수 있도록 도움"],"metadata":{"id":"qdm1AE-zXAlA"}},{"cell_type":"code","source":["# 허깅페이스 가입 후 토큰 발행\n","import json\n","import requests"],"metadata":{"id":"d1HJp80Pakjf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 토큰 넣기\n","API_TOKEN = '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'\n","headers = {\"Authorization\": f\"Bearer {API_TOKEN}\"}\n","API_URL = \"https://api-inference.huggingface.co/models/deepset/roberta-base-squad2\""],"metadata":{"id":"LSl_IqKlcq7Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# API에 POST 요청보내기\n","def query(data):\n","    data = json.dumps(data)\n","    response = requests.request(\"POST\", API_URL, headers=headers, data=data)\n","    return json.loads(response.content.decode(\"utf-8\"))"],"metadata":{"id":"qDiFXobLdBPt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data = query(\n","    {\n","        \"inputs\": {\n","            \"question\": \"너의 이름이 뭐니?\",\n","            \"context\": \"나는 서울에 살고 있고 내 이름은 김사과야\"\n","        }\n","    }\n",")"],"metadata":{"id":"fO7I8NUPdXIw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(data)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GMrdjfdId57R","executionInfo":{"status":"ok","timestamp":1720070259695,"user_tz":-540,"elapsed":1241,"user":{"displayName":"Jeongwon Ryu","userId":"18132725682754503053"}},"outputId":"1559b563-e637-4a11-b404-1278f5c556ee"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'score': 0.001044555683620274, 'start': 19, 'end': 23, 'answer': '김사과야'}\n"]}]},{"cell_type":"code","source":["data = query(\n","    {\n","        \"inputs\": {\n","            \"question\": \"What's my name?\",\n","            \"context\": \"My name is Kim and I live in Seoul\"\n","        }\n","    }\n",")"],"metadata":{"id":"UcYJILTUd9Sn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(data)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rYjJLIEaeSGK","executionInfo":{"status":"ok","timestamp":1720070259695,"user_tz":-540,"elapsed":4,"user":{"displayName":"Jeongwon Ryu","userId":"18132725682754503053"}},"outputId":"4b2b7a54-7c14-4609-83ba-a1e08ccf6f5e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'score': 0.8682517409324646, 'start': 11, 'end': 14, 'answer': 'Kim'}\n"]}]},{"cell_type":"markdown","source":["# **5. 트랜스포머(Transformer)**\n","* 2017년 구글이 발표한 [논문(\"Attention is all you need\")](https://arxiv.org/abs/1706.03762)에서 발표된 모델\n","    * https://arxiv.org/abs/1706.03762\n","* seq2seq의 구조인 인코더-디코더를 따르면서도 어텐션(Attention)만으로 구현된 모델\n","* RNN을 사용하지 않고 인코더-디코더 구조를 설계하였음에도 번역 성능에서 RNN보다 월등히 우수한 성능을 보였으며 2017년 이후 지금까지 다양한 분야에서 사용되는 범용적인 모델\n","* 참고사이트: https://wikidocs.net/31379"],"metadata":{"id":"C4KG8bt2eTsv"}},{"cell_type":"markdown","source":["### 5-1. 트랜스포머의 특징\n","* RNN을 사용하지 않지만 seq2seq 모델의 구조처럼 인코더에서 입력 시퀀스를 입력받고, 디코더에서 출력 시퀀스를 출력하는 인코더-디코더 형식을 사용함\n","* seq2seq 모델 구조에서는 인코더와 디코더를 각각 하나의 RNN 모델처럼 사용했다면, 트랜스포머에서는 인코더와 디코더 단위를 N개로 확장하는 구조를 사용(논문에서는 6개씩 사용)"],"metadata":{"id":"YQDT6gaKiLN_"}},{"cell_type":"markdown","source":["### 5-2. 포지셔널 인코딩(Positional Encoding)\n","* 트랜스포머는 단어의 위치 정보를 얻기 위해 각 단어의 임베딩 벡터에 위치 정보들을 더하여 모델의 입력으로 사용\n","* 위치 정보를 가진 값을 만들기 위해 sin, cos 함수를 사용. (pos, i) 형태로 임베딩 벡터의 위치를 나타냄(i는 임베딩 벡터 내의 차원의 인덱스를 의미)\n","* 임베딩 벡터 내의 차원의 인덱스가 짝수인 경우에는 sin 함수의 값을 사용, 홀수인 경우에는 cos 함수의 값을 사용\n","* 각 임베딩 벡터에 포지셔널 인코딩의 값을 더하면 같은 단어라고 하더라도 문장 내의 위치에 따라 트랜스포머의 입력으로 들어가는 임베딩 벡터의 값이 달라짐"],"metadata":{"id":"bDa8366RoX7e"}},{"cell_type":"markdown","source":["### 5-3. 트랜스포머 셀프 어텐션\n","* 어텐션을 스스로 수행하다는 의미\n","* 하나의 문장 내에서 단어 간의 관계를 파악하기 위해 사용하는 어텐션 매커니즘(seq2seq와 동일)\n"],"metadata":{"id":"ajZ7bfZ_pT54"}},{"cell_type":"markdown","source":["### 5-4. 멀티헤드 어텐션\n","* 어텐션에서는 d_model의 차원을 가진 단어 벡터를 num_heads로 나눈 차원으로 어텐션을 수행\n","* 트랜스포머 연구진은 한 번의 어텐션을 하는 것보다 여러 번의 어텐션을 병렬로 사용하는 것이 더 효과적이라고 판단\n","* 병렬 어텐션을 모두 수행하면 모든 어텐션 헤드를 연결하여 모두 연결된 어텐션 헤드 행렬 크기가 (seq_len, d_model)이 됨"],"metadata":{"id":"3UGlIj_ipmDO"}},{"cell_type":"markdown","source":["### 5-5. Position-wise FFNN(Feed Forward Neural Network)\n","* 일반적인 deep neural network의 feed forward 신경망\n","* 각각의 학습 노드가 서로 완전하게 연결된 Fully-Connected NN이라고 해석할 수 있음"],"metadata":{"id":"pWRXMRLCqCn6"}},{"cell_type":"markdown","source":["### 5-6. 잔차 연결과 레이어 정규화\n","* 입력과 출력은 FFNN을 지나기 때문에 동일한 차원을 가지므로 덧셈이 가능\n","* 잔차 연결을 거친 결과에 layer nomalization 과정을 거침\n","* 수식으로 구현된 인코더는 총 num_layers만큼을 순차적으로 처리한 후에 마지막 층의 인코더의 출력을 디코더로 전달하면서 디코더 연산이 시작"],"metadata":{"id":"D6JY447oqZZV"}},{"cell_type":"markdown","source":["### 5-7. 디코더(Decoder)\n","* 디코더도 인코더와 동일하게 임베딩 층과 포지셔널 인코딩을 거친 후에 문장 행렬이 입력\n","* 학습시 교사강요 기법을 사용하여 학습되므로 학습 과정에서는 디코더는 정답 문장에 해당하는 문장 행렬을 한번에 입력\n","* Look-ahead mask 기법을 사용하여 현 시점의 정답이 아니라 이후에 나올 정답 단어들까지 참조하지 않도록 함"],"metadata":{"id":"Sa3JNtHTq29r"}},{"cell_type":"code","source":["# -*- coding: utf-8 -*-\n","\"\"\"Transformer_Korean_Chatbot.ipynb\n","\n","Automatically generated by Colab.\n","\n","Original file is located at\n","    https://colab.research.google.com/drive/1p1Ca20Dd61oDkI9YyLMuDZbr-DK1Jd0A\n","\"\"\"\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","\n","tf.__version__\n","\n","class PositionalEncoding(tf.keras.layers.Layer):\n","  def __init__(self, position, d_model):\n","    super(PositionalEncoding, self).__init__()\n","    self.pos_encoding = self.positional_encoding(position, d_model)\n","\n","  def get_angles(self, position, i, d_model):\n","    angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n","    return position * angles\n","\n","  def positional_encoding(self, position, d_model):\n","    angle_rads = self.get_angles(\n","        position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n","        i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],\n","        d_model=d_model)\n","    sines = tf.math.sin(angle_rads[:, 0::2])\n","\n","    cosines = tf.math.cos(angle_rads[:, 1::2])\n","\n","    angle_rads = np.zeros(angle_rads.shape)\n","    angle_rads[:, 0::2] = sines\n","    angle_rads[:, 1::2] = cosines\n","    pos_encoding = tf.constant(angle_rads)\n","    pos_encoding = pos_encoding[tf.newaxis, ...]\n","\n","    print(pos_encoding.shape)\n","    return tf.cast(pos_encoding, tf.float32)\n","\n","  def call(self, inputs):\n","    return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]\n","\n","def scaled_dot_product_attention(query, key, value, mask):\n","  # query 크기 : (batch_size, num_heads, query의 문장 길이, d_model/num_heads)\n","  # key 크기 : (batch_size, num_heads, key의 문장 길이, d_model/num_heads)\n","  # value 크기 : (batch_size, num_heads, value의 문장 길이, d_model/num_heads)\n","  # padding_mask : (batch_size, 1, 1, key의 문장 길이)\n","\n","  # Q와 K의 곱. 어텐션 스코어 행렬.\n","  matmul_qk = tf.matmul(query, key, transpose_b=True)\n","\n","  # 스케일링\n","  # dk의 루트값으로 나눠준다.\n","  depth = tf.cast(tf.shape(key)[-1], tf.float32)\n","  logits = matmul_qk / tf.math.sqrt(depth)\n","\n","  # 마스킹. 어텐션 스코어 행렬의 마스킹 할 위치에 매우 작은 음수값을 넣는다.\n","  # 매우 작은 값이므로 소프트맥스 함수를 지나면 행렬의 해당 위치의 값은 0이 된다.\n","  if mask is not None:\n","    logits += (mask * -1e9)\n","\n","  # 소프트맥스 함수는 마지막 차원인 key의 문장 길이 방향으로 수행된다.\n","  # attention weight : (batch_size, num_heads, query의 문장 길이, key의 문장 길이)\n","  attention_weights = tf.nn.softmax(logits, axis=-1)\n","\n","  # output : (batch_size, num_heads, query의 문장 길이, d_model/num_heads)\n","  output = tf.matmul(attention_weights, value)\n","\n","  return output, attention_weights\n","\n","class MultiHeadAttention(tf.keras.layers.Layer):\n","\n","  def __init__(self, d_model, num_heads, name=\"multi_head_attention\"):\n","    super(MultiHeadAttention, self).__init__(name=name)\n","    self.num_heads = num_heads\n","    self.d_model = d_model\n","\n","    assert d_model % self.num_heads == 0\n","\n","    # d_model을 num_heads로 나눈 값.\n","    # 논문 기준 : 64\n","    self.depth = d_model // self.num_heads\n","\n","    # WQ, WK, WV에 해당하는 밀집층 정의\n","    self.query_dense = tf.keras.layers.Dense(units=d_model)\n","    self.key_dense = tf.keras.layers.Dense(units=d_model)\n","    self.value_dense = tf.keras.layers.Dense(units=d_model)\n","\n","    # WO에 해당하는 밀집층 정의\n","    self.dense = tf.keras.layers.Dense(units=d_model)\n","\n","  # num_heads 개수만큼 q, k, v를 split하는 함수\n","  def split_heads(self, inputs, batch_size):\n","    inputs = tf.reshape(\n","        inputs, shape=(batch_size, -1, self.num_heads, self.depth))\n","    return tf.transpose(inputs, perm=[0, 2, 1, 3])\n","\n","  def call(self, inputs):\n","    query, key, value, mask = inputs['query'], inputs['key'], inputs[\n","        'value'], inputs['mask']\n","    batch_size = tf.shape(query)[0]\n","\n","    # 1. WQ, WK, WV에 해당하는 밀집층 지나기\n","    # q : (batch_size, query의 문장 길이, d_model)\n","    # k : (batch_size, key의 문장 길이, d_model)\n","    # v : (batch_size, value의 문장 길이, d_model)\n","    # 참고) 인코더(k, v)-디코더(q) 어텐션에서는 query 길이와 key, value의 길이는 다를 수 있다.\n","    query = self.query_dense(query)\n","    key = self.key_dense(key)\n","    value = self.value_dense(value)\n","\n","    # 2. 헤드 나누기\n","    # q : (batch_size, num_heads, query의 문장 길이, d_model/num_heads)\n","    # k : (batch_size, num_heads, key의 문장 길이, d_model/num_heads)\n","    # v : (batch_size, num_heads, value의 문장 길이, d_model/num_heads)\n","    query = self.split_heads(query, batch_size)\n","    key = self.split_heads(key, batch_size)\n","    value = self.split_heads(value, batch_size)\n","\n","    # 3. 스케일드 닷 프로덕트 어텐션. 앞서 구현한 함수 사용.\n","    # (batch_size, num_heads, query의 문장 길이, d_model/num_heads)\n","    scaled_attention, _ = scaled_dot_product_attention(query, key, value, mask)\n","    # (batch_size, query의 문장 길이, num_heads, d_model/num_heads)\n","    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n","\n","    # 4. 헤드 연결(concatenate)하기\n","    # (batch_size, query의 문장 길이, d_model)\n","    concat_attention = tf.reshape(scaled_attention,\n","                                  (batch_size, -1, self.d_model))\n","\n","    # 5. WO에 해당하는 밀집층 지나기\n","    # (batch_size, query의 문장 길이, d_model)\n","    outputs = self.dense(concat_attention)\n","\n","    return outputs\n","\n","def create_padding_mask(x):\n","  mask = tf.cast(tf.math.equal(x, 0), tf.float32)\n","  # (batch_size, 1, 1, key의 문장 길이)\n","  return mask[:, tf.newaxis, tf.newaxis, :]\n","\n","def encoder_layer(dff, d_model, num_heads, dropout, name=\"encoder_layer\"):\n","  inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n","\n","  # 인코더는 패딩 마스크 사용\n","  padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n","\n","  # 멀티-헤드 어텐션 (첫번째 서브층 / 셀프 어텐션)\n","  attention = MultiHeadAttention(\n","      d_model, num_heads, name=\"attention\")({\n","          'query': inputs, 'key': inputs, 'value': inputs, # Q = K = V\n","          'mask': padding_mask # 패딩 마스크 사용\n","      })\n","\n","  # 드롭아웃 + 잔차 연결과 층 정규화\n","  attention = tf.keras.layers.Dropout(rate=dropout)(attention)\n","  attention = tf.keras.layers.LayerNormalization(\n","      epsilon=1e-6)(inputs + attention)\n","\n","  # 포지션 와이즈 피드 포워드 신경망 (두번째 서브층)\n","  outputs = tf.keras.layers.Dense(units=dff, activation='relu')(attention)\n","  outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n","\n","  # 드롭아웃 + 잔차 연결과 층 정규화\n","  outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n","  outputs = tf.keras.layers.LayerNormalization(\n","      epsilon=1e-6)(attention + outputs)\n","\n","  return tf.keras.Model(\n","      inputs=[inputs, padding_mask], outputs=outputs, name=name)\n","\n","def encoder(vocab_size, num_layers, dff,\n","            d_model, num_heads, dropout,\n","            name=\"encoder\"):\n","  inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n","\n","  # 인코더는 패딩 마스크 사용\n","  padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n","\n","  # 포지셔널 인코딩 + 드롭아웃\n","  embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n","  embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n","  embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n","  outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n","\n","  # 인코더를 num_layers개 쌓기\n","  for i in range(num_layers):\n","    outputs = encoder_layer(dff=dff, d_model=d_model, num_heads=num_heads,\n","        dropout=dropout, name=\"encoder_layer_{}\".format(i),\n","    )([outputs, padding_mask])\n","\n","  return tf.keras.Model(\n","      inputs=[inputs, padding_mask], outputs=outputs, name=name)\n","\n","# 디코더의 첫번째 서브층(sublayer)에서 미래 토큰을 Mask하는 함수\n","def create_look_ahead_mask(x):\n","  seq_len = tf.shape(x)[1]\n","  look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n","  padding_mask = create_padding_mask(x) # 패딩 마스크도 포함\n","  return tf.maximum(look_ahead_mask, padding_mask)\n","\n","def decoder_layer(dff, d_model, num_heads, dropout, name=\"decoder_layer\"):\n","  inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n","  enc_outputs = tf.keras.Input(shape=(None, d_model), name=\"encoder_outputs\")\n","\n","  # 디코더는 룩어헤드 마스크(첫번째 서브층)와 패딩 마스크(두번째 서브층) 둘 다 사용.\n","  look_ahead_mask = tf.keras.Input(\n","      shape=(1, None, None), name=\"look_ahead_mask\")\n","  padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n","\n","  # 멀티-헤드 어텐션 (첫번째 서브층 / 마스크드 셀프 어텐션)\n","  attention1 = MultiHeadAttention(\n","      d_model, num_heads, name=\"attention_1\")(inputs={\n","          'query': inputs, 'key': inputs, 'value': inputs, # Q = K = V\n","          'mask': look_ahead_mask # 룩어헤드 마스크\n","      })\n","\n","  # 잔차 연결과 층 정규화\n","  attention1 = tf.keras.layers.LayerNormalization(\n","      epsilon=1e-6)(attention1 + inputs)\n","\n","  # 멀티-헤드 어텐션 (두번째 서브층 / 디코더-인코더 어텐션)\n","  attention2 = MultiHeadAttention(\n","      d_model, num_heads, name=\"attention_2\")(inputs={\n","          'query': attention1, 'key': enc_outputs, 'value': enc_outputs, # Q != K = V\n","          'mask': padding_mask # 패딩 마스크\n","      })\n","\n","  # 드롭아웃 + 잔차 연결과 층 정규화\n","  attention2 = tf.keras.layers.Dropout(rate=dropout)(attention2)\n","  attention2 = tf.keras.layers.LayerNormalization(\n","      epsilon=1e-6)(attention2 + attention1)\n","\n","  # 포지션 와이즈 피드 포워드 신경망 (세번째 서브층)\n","  outputs = tf.keras.layers.Dense(units=dff, activation='relu')(attention2)\n","  outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n","\n","  # 드롭아웃 + 잔차 연결과 층 정규화\n","  outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n","  outputs = tf.keras.layers.LayerNormalization(\n","      epsilon=1e-6)(outputs + attention2)\n","\n","  return tf.keras.Model(\n","      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n","      outputs=outputs,\n","      name=name)\n","\n","def decoder(vocab_size, num_layers, dff,\n","            d_model, num_heads, dropout,\n","            name='decoder'):\n","  inputs = tf.keras.Input(shape=(None,), name='inputs')\n","  enc_outputs = tf.keras.Input(shape=(None, d_model), name='encoder_outputs')\n","\n","  # 디코더는 룩어헤드 마스크(첫번째 서브층)와 패딩 마스크(두번째 서브층) 둘 다 사용.\n","  look_ahead_mask = tf.keras.Input(\n","      shape=(1, None, None), name='look_ahead_mask')\n","  padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n","\n","  # 포지셔널 인코딩 + 드롭아웃\n","  embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n","  embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n","  embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n","  outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n","\n","  # 디코더를 num_layers개 쌓기\n","  for i in range(num_layers):\n","    outputs = decoder_layer(dff=dff, d_model=d_model, num_heads=num_heads,\n","        dropout=dropout, name='decoder_layer_{}'.format(i),\n","    )(inputs=[outputs, enc_outputs, look_ahead_mask, padding_mask])\n","\n","  return tf.keras.Model(\n","      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n","      outputs=outputs,\n","      name=name)\n","\n","def transformer(vocab_size, num_layers, dff,\n","                d_model, num_heads, dropout,\n","                name=\"transformer\"):\n","\n","  # 인코더의 입력\n","  inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n","\n","  # 디코더의 입력\n","  dec_inputs = tf.keras.Input(shape=(None,), name=\"dec_inputs\")\n","\n","  # 인코더의 패딩 마스크\n","  enc_padding_mask = tf.keras.layers.Lambda(\n","      create_padding_mask, output_shape=(1, 1, None),\n","      name='enc_padding_mask')(inputs)\n","\n","  # 디코더의 룩어헤드 마스크(첫번째 서브층)\n","  look_ahead_mask = tf.keras.layers.Lambda(\n","      create_look_ahead_mask, output_shape=(1, None, None),\n","      name='look_ahead_mask')(dec_inputs)\n","\n","  # 디코더의 패딩 마스크(두번째 서브층)\n","  dec_padding_mask = tf.keras.layers.Lambda(\n","      create_padding_mask, output_shape=(1, 1, None),\n","      name='dec_padding_mask')(inputs)\n","\n","  # 인코더의 출력은 enc_outputs. 디코더로 전달된다.\n","  enc_outputs = encoder(vocab_size=vocab_size, num_layers=num_layers, dff=dff,\n","      d_model=d_model, num_heads=num_heads, dropout=dropout,\n","  )(inputs=[inputs, enc_padding_mask]) # 인코더의 입력은 입력 문장과 패딩 마스크\n","\n","  # 디코더의 출력은 dec_outputs. 출력층으로 전달된다.\n","  dec_outputs = decoder(vocab_size=vocab_size, num_layers=num_layers, dff=dff,\n","      d_model=d_model, num_heads=num_heads, dropout=dropout,\n","  )(inputs=[dec_inputs, enc_outputs, look_ahead_mask, dec_padding_mask])\n","\n","  # 다음 단어 예측을 위한 출력층\n","  outputs = tf.keras.layers.Dense(units=vocab_size, name=\"outputs\")(dec_outputs)\n","\n","  return tf.keras.Model(inputs=[inputs, dec_inputs], outputs=outputs, name=name)\n","\n","small_transformer = transformer(\n","    vocab_size = 9000,\n","    num_layers = 4,\n","    dff = 512,\n","    d_model = 128,\n","    num_heads = 4,\n","    dropout = 0.3,\n","    name=\"small_transformer\")\n","\n","tf.keras.utils.plot_model(\n","    small_transformer, to_file='small_transformer.png', show_shapes=True)\n","\n","def loss_function(y_true, y_pred):\n","  y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n","\n","  loss = tf.keras.losses.SparseCategoricalCrossentropy(\n","      from_logits=True, reduction='none')(y_true, y_pred)\n","\n","  mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n","  loss = tf.multiply(loss, mask)\n","\n","  return tf.reduce_mean(loss)\n","\n","class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n","\n","  def __init__(self, d_model, warmup_steps=4000):\n","    super(CustomSchedule, self).__init__()\n","    self.d_model = d_model\n","    self.d_model = tf.cast(self.d_model, tf.float32)\n","    self.warmup_steps = warmup_steps\n","\n","  def __call__(self, step):\n","    step = tf.cast(step, tf.float32)\n","    arg1 = tf.math.rsqrt(step)\n","    arg2 = step * (self.warmup_steps**-1.5)\n","\n","    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n","\n","sample_learning_rate = CustomSchedule(d_model=128)\n","\n","plt.plot(sample_learning_rate(tf.range(200000, dtype=tf.float32)))\n","plt.ylabel(\"Learning Rate\")\n","plt.xlabel(\"Train Step\")\n","\n","\"\"\"# 챗봇 구현\"\"\"\n","\n","import pandas as pd\n","import urllib.request\n","import tensorflow_datasets as tfds\n","import tensorflow as tf\n","import time\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import re\n","\n","urllib.request.urlretrieve(\"https://raw.githubusercontent.com/songys/Chatbot_data/master/ChatbotData.csv\", filename=\"ChatBotData.csv\")\n","\n","train_data = pd.read_csv('ChatBotData.csv')\n","train_data.head()\n","\n","print('챗봇 샘플의 개수 :', len(train_data))\n","\n","print(train_data.isnull().sum())\n","\n","questions = []\n","for sentence in train_data['Q']:\n","    # 구두점에 대해서 띄어쓰기\n","    # ex) 12시 땡! -> 12시 땡 !\n","    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n","    sentence = sentence.strip()\n","    questions.append(sentence)\n","\n","answers = []\n","for sentence in train_data['A']:\n","    # 구두점에 대해서 띄어쓰기\n","    # ex) 12시 땡! -> 12시 땡 !\n","    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n","    sentence = sentence.strip()\n","    answers.append(sentence)\n","\n","len(questions)\n","\n","print(questions[:5])\n","print(answers[:5])\n","\n","# 서브워드텍스트인코더를 사용하여 질문과 답변을 모두 포함한 단어 집합(Vocabulary) 생성\n","tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n","    questions + answers, target_vocab_size=2**13)\n","\n","# 시작 토큰과 종료 토큰에 대한 정수 부여.\n","START_TOKEN, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]\n","\n","# 시작 토큰과 종료 토큰을 고려하여 단어 집합의 크기를 + 2\n","VOCAB_SIZE = tokenizer.vocab_size + 2\n","\n","print('시작 토큰 번호 :',START_TOKEN)\n","print('종료 토큰 번호 :',END_TOKEN)\n","print('단어 집합의 크기 :',VOCAB_SIZE)\n","\n","# 서브워드텍스트인코더 토크나이저의 .encode()를 사용하여 텍스트 시퀀스를 정수 시퀀스로 변환.\n","print('Tokenized sample question: {}'.format(tokenizer.encode(questions[20])))\n","\n","# 서브워드텍스트인코더 토크나이저의 .encode()와 decode() 테스트해보기\n","\n","# 임의의 입력 문장을 sample_string에 저장\n","sample_string = questions[20]\n","\n","# encode() : 텍스트 시퀀스 --> 정수 시퀀스\n","tokenized_string = tokenizer.encode(sample_string)\n","print ('정수 인코딩 후의 문장 {}'.format(tokenized_string))\n","\n","# decode() : 정수 시퀀스 --> 텍스트 시퀀스\n","original_string = tokenizer.decode(tokenized_string)\n","print ('기존 문장: {}'.format(original_string))\n","\n","# 각 정수는 각 단어와 어떻게 mapping되는지 병렬로 출력\n","# 서브워드텍스트인코더는 의미있는 단위의 서브워드로 토크나이징한다. 띄어쓰기 단위 X 형태소 분석 단위 X\n","for ts in tokenized_string:\n","  print ('{} ----> {}'.format(ts, tokenizer.decode([ts])))\n","\n","# 최대 길이를 40으로 정의\n","MAX_LENGTH = 40\n","\n","# 토큰화 / 정수 인코딩 / 시작 토큰과 종료 토큰 추가 / 패딩\n","def tokenize_and_filter(inputs, outputs):\n","  tokenized_inputs, tokenized_outputs = [], []\n","\n","  for (sentence1, sentence2) in zip(inputs, outputs):\n","    # encode(토큰화 + 정수 인코딩), 시작 토큰과 종료 토큰 추가\n","    sentence1 = START_TOKEN + tokenizer.encode(sentence1) + END_TOKEN\n","    sentence2 = START_TOKEN + tokenizer.encode(sentence2) + END_TOKEN\n","\n","    tokenized_inputs.append(sentence1)\n","    tokenized_outputs.append(sentence2)\n","\n","  # 패딩\n","  tokenized_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n","      tokenized_inputs, maxlen=MAX_LENGTH, padding='post')\n","  tokenized_outputs = tf.keras.preprocessing.sequence.pad_sequences(\n","      tokenized_outputs, maxlen=MAX_LENGTH, padding='post')\n","\n","  return tokenized_inputs, tokenized_outputs\n","\n","questions, answers = tokenize_and_filter(questions, answers)\n","\n","print('질문 데이터의 크기(shape) :', questions.shape)\n","print('답변 데이터의 크기(shape) :', answers.shape)\n","\n","# 0번째 샘플을 임의로 출력\n","print(questions[0])\n","print(answers[0])\n","\n","print('단어 집합의 크기(Vocab size): {}'.format(VOCAB_SIZE))\n","print('전체 샘플의 수(Number of samples): {}'.format(len(questions)))\n","\n","# 텐서플로우 dataset을 이용하여 셔플(shuffle)을 수행하되, 배치 크기로 데이터를 묶는다.\n","# 또한 이 과정에서 교사 강요(teacher forcing)을 사용하기 위해서 디코더의 입력과 실제값 시퀀스를 구성한다.\n","BATCH_SIZE = 64\n","BUFFER_SIZE = 20000\n","\n","# 디코더의 실제값 시퀀스에서는 시작 토큰을 제거해야 한다.\n","dataset = tf.data.Dataset.from_tensor_slices((\n","    {\n","        'inputs': questions,\n","        'dec_inputs': answers[:, :-1] # 디코더의 입력. 마지막 패딩 토큰이 제거된다.\n","    },\n","    {\n","        'outputs': answers[:, 1:]  # 맨 처음 토큰이 제거된다. 다시 말해 시작 토큰이 제거된다.\n","    },\n","))\n","\n","dataset = dataset.cache()\n","dataset = dataset.shuffle(BUFFER_SIZE)\n","dataset = dataset.batch(BATCH_SIZE)\n","dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n","\n","# 임의의 샘플에 대해서 [:, :-1]과 [:, 1:]이 어떤 의미를 가지는지 테스트해본다.\n","print(answers[0]) # 기존 샘플\n","print(answers[:1][:, :-1]) # 마지막 패딩 토큰 제거하면서 길이가 39가 된다.\n","print(answers[:1][:, 1:]) # 맨 처음 토큰이 제거된다. 다시 말해 시작 토큰이 제거된다. 길이는 역시 39가 된다.\n","\n","tf.keras.backend.clear_session()\n","\n","# Hyper-parameters\n","NUM_LAYERS = 2\n","D_MODEL = 256\n","NUM_HEADS = 8\n","DFF = 512\n","DROPOUT = 0.1\n","\n","model = transformer(\n","    vocab_size=VOCAB_SIZE,\n","    num_layers=NUM_LAYERS,\n","    dff=DFF,\n","    d_model=D_MODEL,\n","    num_heads=NUM_HEADS,\n","    dropout=DROPOUT)\n","\n","MAX_LENGTH = 40\n","\n","learning_rate = CustomSchedule(D_MODEL)\n","\n","optimizer = tf.keras.optimizers.Adam(\n","    learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n","\n","def accuracy(y_true, y_pred):\n","  # ensure labels have shape (batch_size, MAX_LENGTH - 1)\n","  y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n","  return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n","\n","model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])\n","\n","EPOCHS = 50\n","\n","model.fit(dataset, epochs=EPOCHS)\n","\n","def evaluate(sentence):\n","  sentence = preprocess_sentence(sentence)\n","\n","  sentence = tf.expand_dims(\n","      START_TOKEN + tokenizer.encode(sentence) + END_TOKEN, axis=0)\n","\n","  output = tf.expand_dims(START_TOKEN, 0)\n","\n","  # 디코더의 예측 시작\n","  for i in range(MAX_LENGTH):\n","    predictions = model(inputs=[sentence, output], training=False)\n","\n","    # 현재(마지막) 시점의 예측 단어를 받아온다.\n","    predictions = predictions[:, -1:, :]\n","    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n","\n","    # 만약 마지막 시점의 예측 단어가 종료 토큰이라면 예측을 중단\n","    if tf.equal(predicted_id, END_TOKEN[0]):\n","      break\n","\n","    # 마지막 시점의 예측 단어를 출력에 연결한다.\n","    # 이는 for문을 통해서 디코더의 입력으로 사용될 예정이다.\n","    output = tf.concat([output, predicted_id], axis=-1)\n","\n","  return tf.squeeze(output, axis=0)\n","\n","\n","def predict(sentence):\n","  prediction = evaluate(sentence)\n","\n","  predicted_sentence = tokenizer.decode(\n","      [i for i in prediction if i < tokenizer.vocab_size])\n","\n","  print('Input: {}'.format(sentence))\n","  print('Output: {}'.format(predicted_sentence))\n","\n","  return predicted_sentence\n","\n","def preprocess_sentence(sentence):\n","  sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n","  sentence = sentence.strip()\n","  return sentence\n","\n","output = predict('영화 볼래?')\n","\n","output = predict(\"고민이 있어\")\n","\n","output = predict(\"너무 화가나\")"],"metadata":{"id":"q_JZJziBrWXU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### [NLP] 자연어 작업 종류\n","* https://dadev.tistory.com/entry/NLP-%EC%9E%90%EC%97%B0%EC%96%B4-%EC%9E%91%EC%97%85-%EC%A2%85%EB%A5%98"],"metadata":{"id":"WhebwuYAfU3B"}},{"cell_type":"markdown","source":["# **6. BERT(Bidirectional Encoder Representations from Transformers)**\n","* 2018년도 구굴의 [논문](https://arxiv.org/abs/1810.04805)에서 제안한 모델로 Transformer의 인코더 기반의 언어 모델\n","    * https://arxiv.org/abs/1810.04805\n","* 버트는 unlabeled data로 부터 pre-train을 진행한 후, 특정 downstream task에 fine-tuning을 하는 모델\n","    * downstream task: 주어진 문제나 작업에 특정하게 맞추어진 task를 의미\n","    * fine-tuning: 사전 학습된 모델을 새로운 작업 또는 데이터셋에 맞게 조정하는 과정\n","* deep bidirectional을 더욱 강조하여 기존의 모델들과의 차별성을 강조\n","* 하나의 output layer만을 pre-trained BERT 모델에 추가하여 NLP의 다양한 주요(11개)에서 SOTA를 달성\n","* 참고 : https://wikidocs.net/109251"],"metadata":{"id":"Pv97fU4xRbAq"}},{"cell_type":"markdown","source":["### 6-1. BERT 모델의 구조\n","* pre-training part와 fine-tuning part로 나눠짐\n","* pre-training 에서는 다양한 pre-training tasks의 unlabeled data를 활용해 파라미터를 조정하고 이를 바탕으로 학습된 모델은 fine-tuning에서 downstream tasks의 labeled data를 이용함\n","* 양방향 transformer encode를 여러층 쌓은 것"],"metadata":{"id":"Tu5P7xufgm8c"}},{"cell_type":"markdown","source":["### 6-2. BERT의 사전 학습\n","* MLM(Masked Language Modeling)\n","    * input tokens의 일정 비율을 마스킹하고 마스킹 된 토큰을 예측하는 과정([MASK])\n","    * pre-training과 fine-tuning 사이의 mismatch가 발생할 수 있음(마스크 토큰이 fine-tuning 과정에서는 나타나지 않게 추가적인 처리가 필요)\n","* NSP(Next Sentence Prediction)\n","    * downstream task 두 문장 사이의 연속성을 확인하는 것이 핵심\n","    * 문장 A와 B를 선택할 때 50%는 실제 A의 다음 문장인 B를 고르고, 나머지 50%는 랜덤 문장 B에서 고르게 함"],"metadata":{"id":"aHcMec4DhUK1"}},{"cell_type":"markdown","source":["* https://github.com/songys/Chatbot_data"],"metadata":{"id":"Jmgi3bLPiBYn"}},{"cell_type":"code","source":["import urllib.request\n","import pandas as pd"],"metadata":{"id":"CDyhr9Ahiy5e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 데이터 불러오기\n","urllib.request.urlretrieve('https://raw.githubusercontent.com/songys/Chatbot_data/master/ChatbotData.csv', filename='ChatBotData.csv')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DoDYThoajBIk","executionInfo":{"status":"ok","timestamp":1720148400943,"user_tz":-540,"elapsed":528,"user":{"displayName":"Jeongwon Ryu","userId":"18132725682754503053"}},"outputId":"97ca8afc-a8a0-461c-9bac-ad62795663fb"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["('ChatBotData.csv', <http.client.HTTPMessage at 0x7b6375bc2bf0>)"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["# 훈련 데이터 확인\n","train_dataset = pd.read_csv('ChatBotData.csv')\n","print(len(train_dataset))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kspYAGsxjF79","executionInfo":{"status":"ok","timestamp":1720148402227,"user_tz":-540,"elapsed":2,"user":{"displayName":"Jeongwon Ryu","userId":"18132725682754503053"}},"outputId":"3a406ad1-6e11-438c-a161-35cf0dde7afb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["11823\n"]}]},{"cell_type":"code","source":["train_dataset"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":423},"id":"juY6wwZxjMjv","executionInfo":{"status":"ok","timestamp":1720148403893,"user_tz":-540,"elapsed":492,"user":{"displayName":"Jeongwon Ryu","userId":"18132725682754503053"}},"outputId":"5ca801ad-979e-4c01-fc9c-538652276ff0"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                             Q                         A  label\n","0                       12시 땡!                하루가 또 가네요.      0\n","1                  1지망 학교 떨어졌어                 위로해 드립니다.      0\n","2                 3박4일 놀러가고 싶다               여행은 언제나 좋죠.      0\n","3              3박4일 정도 놀러가고 싶다               여행은 언제나 좋죠.      0\n","4                      PPL 심하네                눈살이 찌푸려지죠.      0\n","...                        ...                       ...    ...\n","11818           훔쳐보는 것도 눈치 보임.        티가 나니까 눈치가 보이는 거죠!      2\n","11819           훔쳐보는 것도 눈치 보임.             훔쳐보는 거 티나나봐요.      2\n","11820              흑기사 해주는 짝남.                    설렜겠어요.      2\n","11821  힘든 연애 좋은 연애라는게 무슨 차이일까?  잘 헤어질 수 있는 사이 여부인 거 같아요.      2\n","11822               힘들어서 결혼할까봐        도피성 결혼은 하지 않길 바라요.      2\n","\n","[11823 rows x 3 columns]"],"text/html":["\n","  <div id=\"df-08392b79-0d0a-4c8c-bf81-069098b74e1d\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Q</th>\n","      <th>A</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>12시 땡!</td>\n","      <td>하루가 또 가네요.</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1지망 학교 떨어졌어</td>\n","      <td>위로해 드립니다.</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3박4일 놀러가고 싶다</td>\n","      <td>여행은 언제나 좋죠.</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3박4일 정도 놀러가고 싶다</td>\n","      <td>여행은 언제나 좋죠.</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>PPL 심하네</td>\n","      <td>눈살이 찌푸려지죠.</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>11818</th>\n","      <td>훔쳐보는 것도 눈치 보임.</td>\n","      <td>티가 나니까 눈치가 보이는 거죠!</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>11819</th>\n","      <td>훔쳐보는 것도 눈치 보임.</td>\n","      <td>훔쳐보는 거 티나나봐요.</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>11820</th>\n","      <td>흑기사 해주는 짝남.</td>\n","      <td>설렜겠어요.</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>11821</th>\n","      <td>힘든 연애 좋은 연애라는게 무슨 차이일까?</td>\n","      <td>잘 헤어질 수 있는 사이 여부인 거 같아요.</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>11822</th>\n","      <td>힘들어서 결혼할까봐</td>\n","      <td>도피성 결혼은 하지 않길 바라요.</td>\n","      <td>2</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>11823 rows × 3 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-08392b79-0d0a-4c8c-bf81-069098b74e1d')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-08392b79-0d0a-4c8c-bf81-069098b74e1d button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-08392b79-0d0a-4c8c-bf81-069098b74e1d');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-b2e153f9-43d8-4b28-a243-a696065aca19\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b2e153f9-43d8-4b28-a243-a696065aca19')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-b2e153f9-43d8-4b28-a243-a696065aca19 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","  <div id=\"id_bf8bfef1-e81d-43bf-ab3a-ccb1241b554a\">\n","    <style>\n","      .colab-df-generate {\n","        background-color: #E8F0FE;\n","        border: none;\n","        border-radius: 50%;\n","        cursor: pointer;\n","        display: none;\n","        fill: #1967D2;\n","        height: 32px;\n","        padding: 0 0 0 0;\n","        width: 32px;\n","      }\n","\n","      .colab-df-generate:hover {\n","        background-color: #E2EBFA;\n","        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","        fill: #174EA6;\n","      }\n","\n","      [theme=dark] .colab-df-generate {\n","        background-color: #3B4455;\n","        fill: #D2E3FC;\n","      }\n","\n","      [theme=dark] .colab-df-generate:hover {\n","        background-color: #434B5C;\n","        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","        fill: #FFFFFF;\n","      }\n","    </style>\n","    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('train_dataset')\"\n","            title=\"Generate code using this dataframe.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n","  </svg>\n","    </button>\n","    <script>\n","      (() => {\n","      const buttonEl =\n","        document.querySelector('#id_bf8bfef1-e81d-43bf-ab3a-ccb1241b554a button.colab-df-generate');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      buttonEl.onclick = () => {\n","        google.colab.notebook.generateWithVariable('train_dataset');\n","      }\n","      })();\n","    </script>\n","  </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"train_dataset","summary":"{\n  \"name\": \"train_dataset\",\n  \"rows\": 11823,\n  \"fields\": [\n    {\n      \"column\": \"Q\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 11662,\n        \"samples\": [\n          \"\\uc0ac\\ub791\\ud558\\ub294 \\uc0ac\\ub78c \\uc78a\\ub294 \\ubc95\",\n          \"\\uc220 \\uc548 \\uba39\\uc73c\\uba74 \\uce5c\\uad6c\\ub791 \\ubb50\\ud558\\uc9c0\",\n          \"\\uc9dd\\ub0a8\\uc774 \\uace0\\uc2dc\\uc0dd\\uc774\\uba74 \\uae30\\ub2e4\\ub824\\uc57c \\ud558\\ub098\\uc694?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"A\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 7779,\n        \"samples\": [\n          \"\\uc720\\uba38\\ucf54\\ub4dc\\uac00 \\ub9de\\ub294 \\uc0ac\\ub78c\\uc744 \\ucc3e\\uc544\\ubcf4\\uc138\\uc694.\",\n          \"\\uc5ec\\ud589\\uc744 \\ub5a0\\ub098 \\ubcf4\\uc138\\uc694.\",\n          \"\\ud589\\ubcf5\\ud560 \\uac70\\ub77c \\uc0dd\\uac01\\ud574\\uc694.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 2,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0,\n          1,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["# 결측값 확인 삭제\n","train_dataset.replace('', float('NaN'), inplace=True)\n","print(train_dataset.isnull().values.any())"],"metadata":{"id":"s-zuVwfZjOYV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1720148405688,"user_tz":-540,"elapsed":364,"user":{"displayName":"Jeongwon Ryu","userId":"18132725682754503053"}},"outputId":"8806b7dd-e85b-4535-a1e6-7a49db437f76"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["False\n"]}]},{"cell_type":"code","source":["train_dataset = train_dataset.drop_duplicates(['Q']).reset_index(drop=True)\n","print(len(train_dataset))"],"metadata":{"id":"No1PIYDDjl-S","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1720148407036,"user_tz":-540,"elapsed":3,"user":{"displayName":"Jeongwon Ryu","userId":"18132725682754503053"}},"outputId":"ed9f06aa-7e76-4a23-cd04-fa161cf456a5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["11662\n"]}]},{"cell_type":"code","source":["train_dataset = train_dataset.drop_duplicates(['A']).reset_index(drop=True)\n","print(len(train_dataset))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8q1_D1Yujzsq","executionInfo":{"status":"ok","timestamp":1720148408237,"user_tz":-540,"elapsed":1,"user":{"displayName":"Jeongwon Ryu","userId":"18132725682754503053"}},"outputId":"9b3d2466-ef81-4eea-d2be-834882a8abdd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["7731\n"]}]},{"cell_type":"code","source":["import matplotlib.pyplot as plt"],"metadata":{"id":"iQE42y7Vj5Wn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["question_list = list(train_dataset['Q'])\n","answer_list = list(train_dataset['A'])"],"metadata":{"id":"jgjXL8q2kF2C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 최대 / 평균 길이\n","print('질문의 최대 길이: ', max(len(question) for question in question_list))\n","print('질문의 평균 길이: ', sum(map(len, question_list)) / len(question_list))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g5RY64DPkNDZ","executionInfo":{"status":"ok","timestamp":1720148410989,"user_tz":-540,"elapsed":2,"user":{"displayName":"Jeongwon Ryu","userId":"18132725682754503053"}},"outputId":"4e729d33-cd4f-422e-bcd3-7c8e3e662109"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["질문의 최대 길이:  56\n","질문의 평균 길이:  13.6732634846721\n"]}]},{"cell_type":"code","source":["print('답변의 최대 길이: ', max(len(answer) for answer in answer_list))\n","print('답변의 평균 길이: ', sum(map(len, answer_list)) / len(answer_list))"],"metadata":{"id":"uTxAuJRxkaFJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1720148411355,"user_tz":-540,"elapsed":2,"user":{"displayName":"Jeongwon Ryu","userId":"18132725682754503053"}},"outputId":"f9dd3502-507b-483e-fa09-147af26c3645"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["답변의 최대 길이:  76\n","답변의 평균 길이:  15.611563833915406\n"]}]},{"cell_type":"code","source":["import random"],"metadata":{"id":"Tzqt83kTk0Zn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["response_candidates = random.sample(answer_list, 500)"],"metadata":{"id":"4rfLkKe8kk0c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["response_candidates[:10]"],"metadata":{"id":"BZpS_Ksok3UO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1720148412681,"user_tz":-540,"elapsed":3,"user":{"displayName":"Jeongwon Ryu","userId":"18132725682754503053"}},"outputId":"0499f86f-978a-4782-8980-192fec8878c9"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['제 선물 사오세요.',\n"," '그래도 먹으려고 노력해보세요.',\n"," '시간이 벌써 흘렀네요.',\n"," '원하는 꼰대가 있죠.',\n"," '맞춰가기도 하죠.',\n"," '사람마다 다르겠죠.',\n"," '누구나 그럴 거예요.',\n"," '이별이 새로운 시작이 되기도 하니까요.',\n"," '연락이 안되면 썸이 아닐지도 모르겠네요.',\n"," '최선을 다했다면 그 시간 자체를 즐기면 돼요.']"]},"metadata":{},"execution_count":15}]},{"cell_type":"markdown","source":["* SKTBrain에서 공개한 한국어 데이터로 사전학습한 [BERT 모델](https://github.com/monologg/KoBERT-Transformers)"],"metadata":{"id":"j7KlPKLWlepV"}},{"cell_type":"code","source":["!pip install kobert-transformers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EOOMrZkok4uq","executionInfo":{"status":"ok","timestamp":1720148497356,"user_tz":-540,"elapsed":82562,"user":{"displayName":"Jeongwon Ryu","userId":"18132725682754503053"}},"outputId":"772d1719-9c5f-4760-af70-5d1e87c53269"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting kobert-transformers\n","  Downloading kobert_transformers-0.5.1-py3-none-any.whl (12 kB)\n","Requirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from kobert-transformers) (2.3.0+cu121)\n","Requirement already satisfied: transformers<5,>=3 in /usr/local/lib/python3.10/dist-packages (from kobert-transformers) (4.41.2)\n","Requirement already satisfied: sentencepiece>=0.1.91 in /usr/local/lib/python3.10/dist-packages (from kobert-transformers) (0.1.99)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->kobert-transformers) (3.15.4)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->kobert-transformers) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->kobert-transformers) (1.12.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->kobert-transformers) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->kobert-transformers) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->kobert-transformers) (2023.6.0)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.1.0->kobert-transformers)\n","  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.1.0->kobert-transformers)\n","  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.1.0->kobert-transformers)\n","  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.1.0->kobert-transformers)\n","  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.1.0->kobert-transformers)\n","  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.1.0->kobert-transformers)\n","  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.1.0->kobert-transformers)\n","  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.1.0->kobert-transformers)\n","  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.1.0->kobert-transformers)\n","  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.1.0->kobert-transformers)\n","  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n","Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.1.0->kobert-transformers)\n","  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->kobert-transformers) (2.3.0)\n","Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.1.0->kobert-transformers)\n","  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m46.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers<5,>=3->kobert-transformers) (0.23.4)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5,>=3->kobert-transformers) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers<5,>=3->kobert-transformers) (24.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5,>=3->kobert-transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5,>=3->kobert-transformers) (2024.5.15)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers<5,>=3->kobert-transformers) (2.31.0)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5,>=3->kobert-transformers) (0.19.1)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5,>=3->kobert-transformers) (0.4.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers<5,>=3->kobert-transformers) (4.66.4)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.1.0->kobert-transformers) (2.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers<5,>=3->kobert-transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers<5,>=3->kobert-transformers) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers<5,>=3->kobert-transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers<5,>=3->kobert-transformers) (2024.6.2)\n","Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.1.0->kobert-transformers) (1.3.0)\n","Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, kobert-transformers\n","Successfully installed kobert-transformers-0.5.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105\n"]}]},{"cell_type":"code","source":["import torch\n","from kobert_transformers import get_kobert_model"],"metadata":{"id":"5RWV5zlxlIQH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = get_kobert_model()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":213,"referenced_widgets":["d9d9030f78fd4d5cb8c21db88b9cad12","e2b94fcbc633415989b3e01022c3a6dc","ef7cd9250ec145b191617660452765e3","8bed00e751ac4742ad942f64309cf27c","1c8e69027a564803827ff0815c569346","c7e69a98c3a7405fb7d152bd96652d9f","0c24bef00b664229b485e3ff4acba495","e38ad5696b334c27bad23969f2b7f212","11cb954ed830479dbda9a484a5b68072","85907bd2122f4660b51e34c6699931f7","413de357a37a46a6a8f4d8c55f8df2dd","88a359e344fc457dae489fd0cfcde0a3","c53f8a4eb69041318fc41842c1040c4c","982d122e04bd4c77b8a750c34e79a520","ccb7a08534634f0ea7f9990fbefe25f2","4db6310ce7464c65b95e134d14560128","3d237a73bc0648adad6b109967eb61d8","d8386917c06e4544b040888e357c6279","46870319d44c40d990c64f094ddceaa4","c13c3bf8334b41b491deb6079bffdd54","843fbd8d19f541c0b44276c4731d8835","3ae282ab245348c98a8aaa1e262f8be9"]},"id":"9cizWGK1lxf_","executionInfo":{"status":"ok","timestamp":1720148517005,"user_tz":-540,"elapsed":6995,"user":{"displayName":"Jeongwon Ryu","userId":"18132725682754503053"}},"outputId":"e20a3ac8-0104-4243-fc6c-15fa3197e42c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/426 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d9d9030f78fd4d5cb8c21db88b9cad12"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/369M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"88a359e344fc457dae489fd0cfcde0a3"}},"metadata":{}}]},{"cell_type":"code","source":["model.eval()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MtaLLT8clzDc","executionInfo":{"status":"ok","timestamp":1720148519148,"user_tz":-540,"elapsed":357,"user":{"displayName":"Jeongwon Ryu","userId":"18132725682754503053"}},"outputId":"c59bebe6-2a79-4a3f-ffe7-d4e6a20cec4b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["BertModel(\n","  (embeddings): BertEmbeddings(\n","    (word_embeddings): Embedding(8002, 768, padding_idx=1)\n","    (position_embeddings): Embedding(512, 768)\n","    (token_type_embeddings): Embedding(2, 768)\n","    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (encoder): BertEncoder(\n","    (layer): ModuleList(\n","      (0-11): 12 x BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSdpaSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          (intermediate_act_fn): GELUActivation()\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","  )\n","  (pooler): BertPooler(\n","    (dense): Linear(in_features=768, out_features=768, bias=True)\n","    (activation): Tanh()\n","  )\n",")"]},"metadata":{},"execution_count":19}]},{"cell_type":"code","source":["# 문장에서 토큰들의 인덱스\n","input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n","# 모델이 어떤 토큰을 무시해야 하는지 나타내는 텐서(0: 무시, 1: 고려)\n","attention_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n","# 다중 문장 입력을 다룰 때, 각 토큰이 어떤 문장에 속하는지 구분(0, 1)\n","token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\n","output = model(input_ids, attention_mask, token_type_ids)\n","output"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5lh0G8z6l-Yt","executionInfo":{"status":"ok","timestamp":1720148526958,"user_tz":-540,"elapsed":5666,"user":{"displayName":"Jeongwon Ryu","userId":"18132725682754503053"}},"outputId":"e5984727-e511-49bc-8506-dff01e5fac34"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-0.2461,  0.2428,  0.2590,  ..., -0.4861, -0.0731,  0.0756],\n","         [-0.2478,  0.2420,  0.2552,  ..., -0.4877, -0.0727,  0.0754],\n","         [-0.2472,  0.2420,  0.2561,  ..., -0.4874, -0.0733,  0.0765]],\n","\n","        [[ 0.0768, -0.1234,  0.1534,  ..., -0.2518, -0.2571,  0.1602],\n","         [-0.2419, -0.2821,  0.1962,  ..., -0.0172, -0.2960,  0.3679],\n","         [ 0.0911, -0.1437,  0.3412,  ...,  0.2526, -0.1780,  0.2619]]],\n","       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-0.0903, -0.0444,  0.1579,  ...,  0.1010, -0.0819,  0.0529],\n","        [ 0.0742, -0.0116, -0.6845,  ...,  0.0024, -0.0447,  0.0122]],\n","       grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"]},"metadata":{},"execution_count":20}]},{"cell_type":"code","source":["# Sequence Embeddings: 각 토큰의 대한 임베딩, 의미적 표현\n","# pooler_output: 입력 시퀀스에서 추출한 특징의 요약\n","# hidden_states: 모델 내부의 각 레이어에서의 숨겨진 상태값\n","output[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NCRl-9aDm-H9","executionInfo":{"status":"ok","timestamp":1720148529413,"user_tz":-540,"elapsed":338,"user":{"displayName":"Jeongwon Ryu","userId":"18132725682754503053"}},"outputId":"8eecf208-ddda-446a-d3a3-eec0c7d2f137"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[-0.2461,  0.2428,  0.2590,  ..., -0.4861, -0.0731,  0.0756],\n","         [-0.2478,  0.2420,  0.2552,  ..., -0.4877, -0.0727,  0.0754],\n","         [-0.2472,  0.2420,  0.2561,  ..., -0.4874, -0.0733,  0.0765]],\n","\n","        [[ 0.0768, -0.1234,  0.1534,  ..., -0.2518, -0.2571,  0.1602],\n","         [-0.2419, -0.2821,  0.1962,  ..., -0.0172, -0.2960,  0.3679],\n","         [ 0.0911, -0.1437,  0.3412,  ...,  0.2526, -0.1780,  0.2619]]],\n","       grad_fn=<NativeLayerNormBackward0>)"]},"metadata":{},"execution_count":21}]},{"cell_type":"code","source":["!pip install 'git+https://github.com/SKTBrain/KoBERT.git#egg=kobert_tokenizer&subdirectory=kobert_hf'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z5iLzpq_ogaO","executionInfo":{"status":"ok","timestamp":1720148556865,"user_tz":-540,"elapsed":24617,"user":{"displayName":"Jeongwon Ryu","userId":"18132725682754503053"}},"outputId":"91d3883d-9225-4266-ec2f-46d265260965"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting kobert_tokenizer\n","  Cloning https://github.com/SKTBrain/KoBERT.git to /tmp/pip-install-z2khqp0c/kobert-tokenizer_6abbc084b55d4480a76950b785498ee4\n","  Running command git clone --filter=blob:none --quiet https://github.com/SKTBrain/KoBERT.git /tmp/pip-install-z2khqp0c/kobert-tokenizer_6abbc084b55d4480a76950b785498ee4\n","  Resolved https://github.com/SKTBrain/KoBERT.git to commit 47a69af87928fc24e20f571fe10c3cc9dd9af9a3\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: kobert_tokenizer\n","  Building wheel for kobert_tokenizer (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for kobert_tokenizer: filename=kobert_tokenizer-0.1-py3-none-any.whl size=4633 sha256=02c21d6d8bdf1eef4f3ad14d45fbf32231180a1e807ef7fcc7804d7911944eed\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-njk3d1m2/wheels/e9/1a/3f/a864970e8a169c176befa3c4a1e07aa612f69195907a4045fe\n","Successfully built kobert_tokenizer\n","Installing collected packages: kobert_tokenizer\n","Successfully installed kobert_tokenizer-0.1\n"]}]},{"cell_type":"code","source":["from kobert_tokenizer import KoBERTTokenizer"],"metadata":{"id":"PiyehsJforNl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":169,"referenced_widgets":["53999cc586154e5e982c66c2a4cd314f","a3b81e2a995d452b95edfaf2956e9b48","438c1ffc01894509aff17cfc5962fa12","0d62360e2e0642f993a8416f9a1bce95","1a8d6e73e0a14c2a8619968c3022630d","7fbe2c531f02490d907481ecf9f41028","ba68c8cd5e4647bdabd062c216f43d06","ee34bfc8b7bf4539bd83482257154323","1a60267dd4104c4b882237cb11a42e24","f5b8ca4c4c4d4a84852d6d15fee31a0a","f2379e727569412ebe4389466ccfcdcb","d7cc6c1883cc4afdafe792a66b592262","a44910377c2045089fdc7b3271cf5dc2","3086217282324597b4f5225fc7b36f27","9e0a64701699436680ea7c06a9a5d00a","b75016c391e6446ebd754d8305482fed","573d5fe450bc45378f1b41b9fa00885e","0d2950880f9c4dd5ba6b71ad8f8e07d1","06a8483d6de8406ba4a3448d208fa22f","2a844c5ee31b4774a27bc5fadd5f5185","09f0cfe0269f45b1b37b67419d518d71","f9c1ab4497234ce5b42df1b64e98b74b","5c21506eb8eb4511a1bf52cab78b0bd6","ec03c71d8c7d4130b6f5e854fdb2a11d","bde2727457bb4feaab6b7d3ac4a7a4b1","56ad9e9a9c014bb9b04699296861bdd9","b84e111f2b354895be518d2c2fe861ac","b0c8635fce134b159ff0803b0d7737a8","b865426eba144607a0605ca32f183802","c698040f165144ad878e04535e3df9b3","c33f3518c5cc4c2286f67ecc4377233f","d3b0a46b6e7a40b590545485f99f0d2a","efab1c955bf84eb691295d075973fde9"]},"id":"AnFBExua04VS","executionInfo":{"status":"ok","timestamp":1720148579364,"user_tz":-540,"elapsed":1595,"user":{"displayName":"Jeongwon Ryu","userId":"18132725682754503053"}},"outputId":"a920b05a-79df-4e10-a237-94edac9c101b"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/432 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"53999cc586154e5e982c66c2a4cd314f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["spiece.model:   0%|          | 0.00/371k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d7cc6c1883cc4afdafe792a66b592262"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["special_tokens_map.json:   0%|          | 0.00/244 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c21506eb8eb4511a1bf52cab78b0bd6"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n","The tokenizer class you load from this checkpoint is 'XLNetTokenizer'. \n","The class this function is called from is 'KoBERTTokenizer'.\n"]}]},{"cell_type":"code","source":["tokenizer.tokenize('[CLS] 한국어 모델을 공유합니다. [SEP]')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z1BYDrkinOZP","executionInfo":{"status":"ok","timestamp":1720148587302,"user_tz":-540,"elapsed":401,"user":{"displayName":"Jeongwon Ryu","userId":"18132725682754503053"}},"outputId":"57ca7344-ca1a-4ee6-a48b-9eb5ff981749"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['[CLS]', '▁한국', '어', '▁모델', '을', '▁공유', '합니다', '.', '[SEP]']"]},"metadata":{},"execution_count":26}]},{"cell_type":"code","source":["tokenizer.convert_tokens_to_ids(['[CLS]', '▁한국', '어', '▁모델', '을', '▁공유', '합니다', '.', '[SEP]'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3wh9El9GoD8L","executionInfo":{"status":"ok","timestamp":1720148594336,"user_tz":-540,"elapsed":335,"user":{"displayName":"Jeongwon Ryu","userId":"18132725682754503053"}},"outputId":"1b70f87d-db0a-4ee6-8b1a-a5ad10c65a80"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[2, 4958, 6855, 2046, 7088, 1050, 7843, 54, 3]"]},"metadata":{},"execution_count":27}]},{"cell_type":"code","source":["import numpy as np\n","import torch\n","from sklearn.metrics.pairwise import cosine_similarity"],"metadata":{"id":"Zqs4RQW7oHhA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_cls_token(sentence):\n","    model.eval()\n","    tokenized_sent = tokenizer(\n","        sentence,\n","        return_tensors='pt',\n","        truncation=True,\n","        add_special_tokens=True,\n","        max_length=128\n","    )\n","\n","    input_ids = tokenized_sent['input_ids']\n","    attention_mask = tokenized_sent['attention_mask']\n","    token_type_ids = tokenized_sent['token_type_ids']\n","\n","    with torch.no_grad():\n","        output = model(input_ids, attention_mask, token_type_ids)\n","\n","    cls_output = output[1]\n","    cls_token = cls_output.detach().cpu().numpy()\n","    return cls_token"],"metadata":{"id":"DX66o0Gf2Klr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["get_cls_token('너 요즘 바뻐?')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_X_YJQiZ2MQw","executionInfo":{"status":"ok","timestamp":1720148614723,"user_tz":-540,"elapsed":356,"user":{"displayName":"Jeongwon Ryu","userId":"18132725682754503053"}},"outputId":"1ef7d399-d83c-4515-b2b5-e85df93fe488"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[ 1.97710767e-02, -4.20350768e-02, -8.08144733e-02,\n","         1.43184122e-02, -8.87244165e-01,  9.76347744e-01,\n","        -3.12680244e-01,  1.08308986e-01, -7.70816207e-02,\n","        -7.28073716e-02, -5.49437523e-01, -1.36874672e-02,\n","        -4.49552620e-03,  1.16062745e-01, -6.47504628e-03,\n","         6.94795072e-01, -9.89945292e-01,  6.73852935e-02,\n","        -4.00664769e-02, -2.03488693e-02,  6.67853504e-02,\n","        -2.53232615e-03, -4.10423279e-02,  5.43793619e-01,\n","         3.53782475e-02,  7.47534811e-01, -9.01233196e-01,\n","        -9.19028372e-02, -8.82182419e-01,  9.46916267e-02,\n","         9.30823743e-01,  6.68539882e-01, -4.72913533e-02,\n","        -4.87458222e-02, -8.58902276e-01, -1.67718083e-01,\n","        -3.68322767e-02, -1.58500206e-02, -9.75222468e-01,\n","        -5.68215884e-02, -4.96405102e-02,  2.40126927e-03,\n","         3.73748504e-02,  9.78760481e-01, -9.96391356e-01,\n","         6.99635297e-02, -5.84157288e-01,  2.57412475e-02,\n","        -9.95675206e-01,  9.74204123e-01, -2.00892389e-01,\n","        -9.99807358e-01, -9.80349660e-01,  1.94251556e-02,\n","        -7.49888346e-02, -7.94396877e-01,  1.02787055e-01,\n","         6.21608555e-01, -1.45015549e-02,  8.89343321e-02,\n","         1.19151324e-02,  5.47532402e-02,  9.20245588e-01,\n","        -9.87935603e-01,  3.01291365e-02, -2.65659075e-02,\n","        -1.06672801e-01, -3.09796333e-02,  9.87813156e-03,\n","         1.64681040e-02,  9.40921664e-01, -1.11120893e-02,\n","         2.79974759e-01, -6.98985979e-02, -5.59643269e-01,\n","        -2.04323027e-02, -1.22432806e-01, -1.63028762e-02,\n","         1.21791422e-01,  5.74307814e-02,  1.61552634e-02,\n","        -9.55803871e-01, -5.35651982e-01,  4.11142021e-01,\n","        -4.79983501e-02,  5.45244999e-02,  6.68845400e-02,\n","        -5.60482778e-03,  9.91440356e-01, -6.08556330e-01,\n","        -2.80755088e-02, -1.51853878e-02,  4.41351086e-02,\n","         4.45505381e-02, -9.37977433e-01,  3.68433371e-02,\n","        -3.91985960e-02,  3.95909175e-02,  1.30552910e-02,\n","         8.80396903e-01,  2.85759736e-02, -6.91244975e-02,\n","        -4.83701080e-02,  7.50415549e-02, -9.57032442e-01,\n","         3.85180935e-02, -4.11867462e-02,  7.57847168e-03,\n","        -2.58947182e-02,  2.30287239e-02, -6.81484304e-03,\n","        -4.77750376e-02,  9.02029037e-01,  5.78373745e-02,\n","        -5.13116121e-01,  4.66836095e-02, -1.21095860e-02,\n","        -7.28293881e-02,  8.30237940e-03,  7.80004859e-02,\n","         1.20663293e-01,  6.35053292e-02, -9.91669595e-01,\n","         8.69435728e-01, -9.99854863e-01, -9.14239697e-03,\n","        -8.98460686e-01,  3.41067724e-02, -1.39556676e-02,\n","        -2.11769086e-03,  5.12873158e-02,  3.34920660e-02,\n","         1.09836021e-02,  4.99967225e-02,  6.09446391e-02,\n","         4.96051274e-02, -9.28290963e-01, -5.49920425e-02,\n","         8.07364658e-02,  7.13708028e-02,  3.20894457e-02,\n","         5.07209823e-02,  3.52416933e-01,  9.20597017e-01,\n","        -1.33979255e-02,  2.50120778e-02,  1.47374803e-02,\n","         7.09045902e-02, -1.00760655e-02,  9.00733843e-03,\n","        -9.46521580e-01,  6.42014667e-02, -8.47015619e-01,\n","         2.35850960e-02,  1.11310289e-03,  3.85401472e-02,\n","         8.96188691e-02, -3.28979567e-02, -2.55993195e-02,\n","        -8.79032314e-01,  7.56769553e-02,  4.37906086e-02,\n","        -2.70840991e-02,  9.90387022e-01, -1.68379117e-02,\n","         1.21676795e-01,  5.42990081e-02,  4.73825306e-01,\n","        -3.73175219e-02,  9.82727110e-03, -5.52667044e-02,\n","        -8.36800877e-03, -9.21916604e-01, -7.08967865e-01,\n","         1.21669941e-01,  9.99682784e-01, -1.97477359e-02,\n","        -8.77339661e-01,  3.16802524e-02, -1.83017738e-02,\n","        -5.06127905e-03,  9.76287365e-01,  2.90160269e-01,\n","        -9.99827921e-01,  3.17350700e-02,  3.96242850e-02,\n","        -1.32721420e-02, -3.51912752e-02,  5.21909967e-02,\n","         1.99957136e-02, -1.75941717e-02,  1.05277732e-01,\n","        -6.90916628e-02, -8.85318518e-02, -1.31769240e-01,\n","        -4.25230265e-02,  2.88106799e-01,  4.25478676e-03,\n","         1.61117427e-02, -6.21332973e-02,  2.75976900e-02,\n","         4.47716653e-01,  2.58004311e-02, -3.00957412e-02,\n","         8.87380242e-01, -3.50626335e-02,  1.04203960e-02,\n","         3.23608667e-02, -5.06322980e-01,  9.14470971e-01,\n","        -2.15907302e-03,  3.48478518e-02,  8.74281049e-01,\n","         8.57236803e-01, -5.60338758e-02,  4.88038436e-02,\n","        -2.96013039e-02,  5.85433304e-01,  8.85599315e-01,\n","        -9.69411731e-01, -8.37742925e-01, -9.04356316e-03,\n","         5.83446678e-03, -2.27307491e-02, -6.48914184e-03,\n","         2.71145493e-01,  7.31240630e-01, -9.81174767e-01,\n","        -3.50157991e-02,  7.24898512e-03, -9.91511464e-01,\n","        -2.15140823e-02,  3.07667535e-02, -2.28972822e-01,\n","         4.12350982e-01, -2.97829509e-01,  2.97682166e-01,\n","        -1.22985616e-02, -4.94737402e-02, -2.75764633e-02,\n","         9.01489854e-01, -8.02897476e-03,  8.63082528e-01,\n","        -8.56264979e-02,  4.14134771e-01,  8.60826373e-02,\n","        -4.89612371e-02,  9.95140016e-01,  1.66754305e-01,\n","         9.85895157e-01,  1.45101501e-02,  5.71455844e-02,\n","        -6.46066070e-02,  2.03103833e-02,  1.82742812e-02,\n","         8.81142765e-02,  9.78763640e-01,  6.46322146e-02,\n","         9.87488270e-01, -8.65316749e-01,  2.63854060e-02,\n","         2.06853002e-02, -4.01426572e-03, -4.11330797e-02,\n","         6.69815615e-02, -2.93425154e-02,  7.46286437e-02,\n","        -4.72679168e-01, -5.91187775e-01,  5.13811931e-02,\n","         2.87640747e-02, -7.29547888e-02,  4.40406464e-02,\n","        -2.94401437e-01,  6.77514911e-01, -8.84745300e-01,\n","         9.12754238e-02,  5.19084837e-03,  7.55682960e-02,\n","         2.60925218e-02, -5.04909791e-02,  1.58709645e-01,\n","        -9.80808735e-01, -3.30211632e-02,  4.70191948e-02,\n","        -2.77179200e-02, -6.82025403e-02, -8.56657550e-02,\n","         5.77225462e-02,  2.73137875e-02,  9.69972074e-01,\n","         7.23451525e-02,  5.73496461e-01, -5.53213619e-02,\n","        -3.43014821e-02, -5.73315583e-02, -3.60760212e-01,\n","         9.92592335e-01, -4.29719985e-02, -2.54403288e-03,\n","        -3.73234949e-03,  1.75964028e-01, -1.12268040e-02,\n","         2.51378701e-03,  9.99624670e-01, -9.35691502e-03,\n","        -5.24946392e-01,  4.96206492e-01,  8.27659667e-02,\n","        -1.16833791e-01, -4.28614020e-01,  3.59543599e-02,\n","        -3.91330495e-02,  9.48929429e-01,  9.99915421e-01,\n","        -6.58797449e-04,  1.53927263e-02,  4.19992581e-02,\n","         9.35816690e-02, -1.50320634e-01, -3.90116917e-03,\n","         1.39817866e-02,  3.35921049e-02,  3.17463502e-02,\n","         6.70463890e-02, -2.85736211e-02, -6.41713440e-01,\n","         1.05719715e-02,  1.15118707e-02,  1.43086966e-02,\n","        -4.05126363e-02, -8.23382381e-03, -8.90454829e-01,\n","        -1.34509414e-01, -9.31037903e-01,  4.82047879e-04,\n","         1.24617824e-02,  3.91753390e-02,  2.55453791e-02,\n","         8.59405994e-01, -2.67724581e-02, -4.12803404e-02,\n","         7.50546694e-01, -9.87331942e-02, -9.46730614e-01,\n","        -9.98687983e-01, -3.62063646e-02,  8.82269025e-01,\n","        -2.75542750e-03,  3.37444767e-02, -3.71668451e-02,\n","        -1.42455339e-01, -3.06474447e-01,  2.31449753e-02,\n","         1.50678470e-03, -4.18973677e-02, -6.22344851e-01,\n","         9.99812782e-01, -5.05036078e-02,  2.84894705e-02,\n","        -3.53689156e-02,  9.13168397e-03, -5.22863269e-01,\n","        -9.89726126e-01,  7.09966421e-02, -2.42635906e-02,\n","        -9.89548385e-01,  6.83806688e-02,  4.07989509e-02,\n","         1.08705768e-02, -4.37120557e-01, -5.63813567e-01,\n","         4.74738106e-02,  3.54076736e-02, -7.31699988e-02,\n","        -4.36486788e-02, -9.32106495e-01,  4.00953561e-01,\n","        -1.69779584e-02,  3.44159156e-02, -9.99760509e-01,\n","        -8.16055462e-02,  9.42515135e-01,  5.87236285e-02,\n","         7.71812722e-02,  2.63806395e-02, -9.58489720e-03,\n","         1.69933997e-02, -3.50716822e-02, -2.74967798e-03,\n","        -7.07111180e-01,  9.99636531e-01, -1.39362827e-01,\n","        -1.13253877e-01, -1.54854683e-02,  2.27557104e-02,\n","        -3.57570089e-02,  4.39596996e-02, -4.65913191e-02,\n","         1.51157388e-02,  8.78140610e-03, -3.01217474e-02,\n","        -1.23014217e-02,  9.32076037e-01, -6.01028323e-01,\n","         9.51776683e-01,  1.83694020e-01,  5.11920787e-02,\n","        -4.39717144e-01,  7.78893381e-02,  2.72272620e-03,\n","        -9.82761145e-01, -9.57825243e-01, -2.36347355e-02,\n","         2.47249044e-02, -5.78276396e-01,  3.44472239e-03,\n","        -9.28244088e-03, -4.05460931e-02, -5.06315604e-02,\n","         1.81933064e-02, -8.90301168e-01,  5.07990494e-02,\n","        -8.97732139e-01,  8.36622238e-01, -9.70130786e-02,\n","        -6.55984804e-02, -5.59527814e-01, -3.37563567e-02,\n","        -1.53997168e-01, -3.12336564e-01, -9.79726203e-03,\n","        -3.76110375e-01, -6.79448247e-01, -5.14979720e-01,\n","         4.50538099e-02,  8.84905756e-01, -2.19864130e-01,\n","         8.45167458e-01,  9.02457714e-01,  5.73191524e-01,\n","        -1.03604347e-02,  4.08219099e-02,  9.99873698e-01,\n","        -5.77480495e-01,  7.31348395e-02, -6.93641603e-02,\n","         5.33510864e-01,  8.34201932e-01,  6.06250986e-02,\n","        -9.22108591e-01, -4.53238487e-02, -9.21295106e-01,\n","        -1.56668026e-03,  1.53639941e-02,  5.07283723e-03,\n","        -6.68961555e-02, -3.83358270e-01, -8.48744344e-03,\n","         6.85555279e-01, -8.37421790e-03, -4.52449918e-02,\n","         5.83142042e-02, -1.70860998e-02, -5.92009239e-02,\n","         9.21067178e-01,  9.18298736e-02, -8.46073806e-01,\n","        -1.67632520e-01, -9.61074114e-01,  1.10302847e-02,\n","        -5.71848564e-02,  4.48717773e-02,  7.93936968e-01,\n","        -2.97717210e-02, -7.98422694e-01, -3.50071192e-02,\n","        -2.32628509e-02,  9.07549977e-01,  1.07618026e-01,\n","        -9.99721169e-01, -9.81233180e-01, -8.30599129e-01,\n","         1.21828774e-02,  7.36793458e-01, -8.75474751e-01,\n","         5.85044026e-02,  3.26856934e-02, -1.54627708e-03,\n","        -9.97475147e-01,  7.12855682e-02, -1.55169787e-02,\n","         2.20191944e-02, -6.73404057e-03, -8.83155704e-01,\n","         3.59170139e-02,  4.71342802e-01,  3.00889350e-02,\n","         2.49722190e-02, -1.51230305e-01, -9.28689122e-01,\n","        -4.23124358e-02, -2.99455486e-02, -6.59474492e-01,\n","        -1.81930587e-02,  2.39341199e-01,  1.19231373e-01,\n","        -7.52601445e-01,  9.88331318e-01,  7.62677751e-03,\n","         7.32321069e-02, -3.86127740e-01, -1.04530351e-02,\n","        -5.44525087e-02, -8.44074011e-01,  8.42035353e-01,\n","         1.12028673e-01, -1.75491180e-02,  2.72480794e-03,\n","        -5.30968495e-02, -4.97388700e-03,  1.58805009e-02,\n","        -3.42542902e-02, -7.41725147e-01, -4.15325314e-02,\n","         7.68830419e-01,  1.60676837e-02, -1.04532279e-02,\n","         3.49776298e-02,  5.09097502e-02, -9.76349711e-01,\n","        -8.50231424e-02, -9.76854920e-01,  2.29703821e-02,\n","         8.29891086e-01,  2.15306822e-02, -5.02061918e-02,\n","         1.86862275e-02, -5.37560545e-02, -9.99018133e-01,\n","        -1.97572820e-02, -9.99930263e-01,  1.22420033e-02,\n","         6.10845387e-01, -9.94518846e-02, -4.60792109e-02,\n","         6.06065691e-01,  2.74290554e-02, -8.19775403e-01,\n","        -8.31642866e-01, -1.38606563e-01, -9.99899149e-01,\n","        -9.27113950e-01, -8.00279319e-01, -2.65903808e-02,\n","         5.39314866e-01,  1.15310010e-02,  5.63823044e-01,\n","         4.38050665e-02, -5.28343581e-02, -2.29863226e-02,\n","        -9.44951773e-02,  2.46265568e-02, -2.15098802e-02,\n","        -8.75434101e-01,  2.61014905e-02,  7.20123351e-02,\n","        -6.39878064e-02,  3.65060531e-02, -4.86739278e-01,\n","        -1.56572321e-04, -2.36862916e-02,  1.02463190e-03,\n","        -9.59654152e-01,  2.87307482e-02,  3.06237512e-03,\n","         1.77779943e-02, -3.77629668e-01,  3.26561294e-02,\n","        -3.10277194e-02,  3.51715609e-02, -9.44145262e-01,\n","        -8.99598226e-02, -1.04208075e-01,  6.93175316e-01,\n","        -7.50540942e-02,  6.90494925e-02, -8.82190049e-01,\n","         2.50913054e-02,  6.54611647e-01,  5.60182929e-01,\n","         7.07554340e-01,  9.60617602e-01,  6.14133663e-02,\n","         5.77285767e-01, -2.83782780e-02, -3.36265355e-01,\n","         3.31813917e-02,  8.99018049e-01,  1.10603735e-01,\n","        -5.64125776e-02,  5.42354956e-02, -9.56353724e-01,\n","        -8.88354331e-02, -9.99193132e-01, -2.48066094e-02,\n","        -9.57827032e-01, -9.72862005e-01,  2.94865482e-02,\n","         3.90532874e-02, -7.86748827e-01, -4.07382220e-01,\n","        -5.29809408e-02, -3.25191990e-02,  2.30216887e-02,\n","        -3.80926370e-03,  5.05167842e-02, -8.55316818e-01,\n","        -2.33480986e-03,  4.45173681e-01, -3.50025967e-02,\n","         1.75281393e-03,  4.00337484e-03, -9.21897665e-02,\n","         3.99733987e-03, -1.36185333e-01, -4.03722115e-02,\n","         9.40170705e-01,  8.97295550e-02, -9.09735024e-01,\n","         7.61119843e-01, -2.85651465e-03, -6.37132943e-01,\n","        -6.98854700e-02, -4.47483547e-02,  4.74469643e-03,\n","         9.08056021e-01, -2.85296943e-02,  9.99938309e-01,\n","         9.73304152e-01, -8.72908533e-01,  5.02233207e-01,\n","         4.86812592e-02,  2.82572865e-01,  6.34035766e-02,\n","        -9.62208569e-01, -5.02870344e-02, -8.60813975e-01,\n","        -6.44549429e-02,  2.71886997e-02,  9.14090276e-02,\n","        -1.74799949e-01, -9.93987024e-01,  3.31036234e-03,\n","         3.66921537e-02, -1.21775888e-01, -8.67323697e-01,\n","        -7.44412318e-02, -9.99465227e-01, -1.81428138e-02,\n","         8.87997985e-01,  8.39023948e-01,  5.19102886e-02,\n","        -1.27681652e-02,  9.94056821e-01,  6.85030282e-01,\n","        -8.53934169e-01,  3.65198217e-02, -1.86118321e-03,\n","        -9.46957245e-03, -9.58351433e-01,  4.58785333e-02,\n","        -7.18918964e-02, -8.44074879e-03,  5.83786406e-02,\n","         3.54283564e-02, -5.40310629e-02,  5.03920138e-01,\n","         1.63402826e-01,  7.98747614e-02,  9.56079736e-02,\n","         8.89034927e-01, -6.37683421e-02, -3.11780199e-02,\n","         5.65440953e-03, -6.76189601e-01,  2.26583462e-02,\n","         9.99777913e-01,  5.42105973e-01,  8.81657779e-01,\n","         9.93905306e-01,  5.51112695e-03,  3.81119363e-02,\n","         8.49793911e-01,  5.89373648e-01, -1.22707905e-02,\n","        -1.56028047e-01, -9.92524326e-01, -2.87819933e-02,\n","        -1.58514604e-01,  9.09859717e-01,  2.19296932e-01,\n","        -9.99814630e-01,  1.32726058e-02,  9.70062315e-01,\n","         9.70911086e-01, -6.44348145e-01,  8.52031708e-01,\n","        -9.63063955e-01, -5.76023340e-01, -5.83867766e-02,\n","         2.64066979e-02, -7.20684826e-01,  2.16148351e-03,\n","        -4.64603119e-03,  9.99902904e-01,  4.17145006e-02,\n","         4.54902463e-02,  3.64560075e-02,  5.55263925e-03,\n","         9.59035009e-03, -7.67510058e-03,  5.07941730e-02,\n","         8.90135884e-01,  2.93286089e-02, -6.64202794e-02,\n","        -2.92305183e-02, -9.98543262e-01, -2.52464890e-01,\n","        -4.86114323e-01, -2.66075898e-02, -1.43679101e-02,\n","        -9.95312929e-01, -5.84235340e-02,  4.28002514e-03,\n","        -8.46151039e-02, -2.91191548e-01,  4.45334353e-02,\n","        -5.85757867e-02,  1.04175664e-01, -3.75008374e-03,\n","        -1.49845146e-02,  1.33885629e-02, -5.59755489e-02,\n","        -6.55457228e-02,  9.34130967e-01, -7.00236917e-01,\n","         4.75338660e-02,  2.88284430e-03,  9.87558126e-01,\n","        -2.82190274e-03,  6.53677732e-02, -5.88703156e-01,\n","        -9.38618481e-01, -8.60266685e-01,  8.97186995e-01,\n","         1.31905805e-02, -6.95280194e-01, -4.05456796e-02,\n","        -9.66625988e-01,  2.19614170e-02,  8.83074164e-01,\n","        -5.53830087e-01, -9.96751338e-03, -9.90602732e-01,\n","         1.55075714e-01, -7.11186454e-02, -2.03615613e-02]], dtype=float32)"]},"metadata":{},"execution_count":30}]},{"cell_type":"code","source":["def predict(query, candidates):\n","    candidates_cls = []\n","\n","    for cand in candidates:\n","        cand_cls = get_cls_token(cand)\n","        candidates_cls.append(cand_cls)\n","\n","    candidates_cls = np.array(candidates_cls).squeeze(axis=1)\n","    query_cls = get_cls_token(query)\n","    similarity_list = cosine_similarity(query_cls, candidates_cls)\n","    target_idx = np.argmax(similarity_list)\n","    return candidates[target_idx]"],"metadata":{"id":"ujNd-d5h2NsA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sample_query = '너 요즘 바뻐?'\n","# print(get_cls_token(sample_query))\n","print(get_cls_token(sample_query).shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ot2xlh_d2Px3","executionInfo":{"status":"ok","timestamp":1720148629533,"user_tz":-540,"elapsed":329,"user":{"displayName":"Jeongwon Ryu","userId":"18132725682754503053"}},"outputId":"405e78dc-b992-4b37-d64a-8e439f29c78c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(1, 768)\n"]}]},{"cell_type":"code","source":["sample_query = '너 요즘 바뻐?'\n","sample_candidates = ['바쁘면 가버려', '아니 안바뻐', '오늘은 이만', '에붸붸붸']\n","\n","predicted_answer = predict('sample_query', sample_candidates)\n","print(f'결과: {predicted_answer}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GCftgri52RT5","executionInfo":{"status":"ok","timestamp":1720148637145,"user_tz":-540,"elapsed":1572,"user":{"displayName":"Jeongwon Ryu","userId":"18132725682754503053"}},"outputId":"d92a8805-69e3-444a-9d8f-077e055b1590"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["결과: 아니 안바뻐\n"]}]},{"cell_type":"code","source":["sample_query = '힘든 연애 좋은 연애라는게 무슨 차이일까?'\n","sample_candidates = random.sample(answer_list, 100)\n","\n","predicted_answer = predict('sample_query', sample_candidates)\n","print(f'결과: {predicted_answer}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8-u50N1w2S3Q","executionInfo":{"status":"ok","timestamp":1720148665628,"user_tz":-540,"elapsed":22255,"user":{"displayName":"Jeongwon Ryu","userId":"18132725682754503053"}},"outputId":"42741753-f209-4b03-eccb-8f24856c04d4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["결과: 너무 늦지 않았길 바라요\n"]}]},{"cell_type":"code","source":["end = 1\n","while end == 1:\n","    sentence = input('질문을 입력하세요: ')\n","    if len(sentence) == 0:\n","        break\n","    predicted_answer = predict(sentence, response_candidates)\n","    print(predicted_answer)\n","    print('\\n')"],"metadata":{"id":"xWPn7tql2Upa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **7. GPT(Generative Pre-Training)**\n","* GPT 모델은 2018년 6월에 OpenAI가 논문을 통해 처음 제안\n","    * [GPT1](http://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf)\n","        * http://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf\n","    * [GPT2](http://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf)\n","        * http://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf\n","    * [GPT3](http://arxiv.org/abs/2005.14165)\n","        * http://arxiv.org/abs/2005.14165\n","    * [GPT4](https://cdn.openai.com/papers/gpt-4.pdf)\n","        * https://cdn.openai.com/papers/gpt-4.pdf\n","* GPT도 unlabeled data로 부터 pre-train을 진행한 후 특정 downstream task에 fine-tuning을 하는 모델\n","* Transformer의 decoder만 사용하는 구조"],"metadata":{"id":"OrEPI3w-zVth"}},{"cell_type":"markdown","source":["### 1-1. GPT 모델의 특징\n","* 사전 학습에는 대규모의 unlabeled data를 사용하는데 unlabeld data에서 단어 수준 이상의 정보를 얻는 것은 매우 힘듦. 또한 어떤 방법이 유용한 텍스트 표현을 배우는데 효과적인지 불분명함\n","* 사전학습 이후에도 어떤 방법이 fine-tuning에 가장 효과적인지 불분명\n","* GPT 논문에서는 unsupervised pre-training과 supervised fine-tuning의 조합을 사용한 접근법을 제안\n","* 모델은 이미 효과가 검증된 2017년 공개된 transformer를 사용\n","* fine-tuning에서의 미세 조정만으로 다양한 자연어처리 작업에 적용할 수 있는 범위적인 표현을 사전학습에서 학습하는 것\n","### GPT 모델 구조\n","* GPT는 Transformer의 변형인 multi-layer Transformer decoder만 사용\n","* 입력 문맥 token에 multi-headed self-attention을 적용한 후, token에 대한 출력 분표를 얻기 위해 position-wise feedforward layer를 적용\n","### GPT 모델의 특징\n","* 모델 구조의 변형이 거의 없음. 이전의 사전 학습 모델들은 fine-tuning할 때 모델 구조를 변형해야 하는 문제점이 있었음\n","* 모델 구조를 변형하지 않고 linear layer를 마지막에 추가하는 아주 간단한 추가적 작업만 수행하면 됨"],"metadata":{"id":"g2PHAxFueFhQ"}},{"cell_type":"markdown","source":["### 1-2. GPT 모델 학습\n","* unsupervised pre-training\n","    * 대규모 코퍼스에서 unsupervised learning으로 언어 모델을 학습\n","    * transformer 디코더를 사용하여 계속 next token prediction 학습라는 것\n","    * multi-layer Transformer decoder를 사용하여 입력 문맥 token에 multi-headed self-attention을 적용한 후, 목표 token에 대한 출력 분포를 얻기 위해 position-wise feedforward layer를 적용\n","* supervised fine-tuning\n","    * 특정 작업에 대한 데이터로 모델을 fine-tuning\n","    * fine-tuning 단계에서는 사전 학습된 모델을 각 task에 맞게 input과 label로 구성된 supervised dataset에 대해 학습\n","    * 결과를 task에 맞는 loss들을 결합\n","\n","> 사전 학습은 next token prediction이라는 language modeling으로 진행되었기 때문에 각 downstream task와 input 모양이 다를 수 밖에 없음"],"metadata":{"id":"JmGP2TLffehJ"}},{"cell_type":"code","source":["!pip install transformers"],"metadata":{"id":"Li7q__tl0PjP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1720664146965,"user_tz":-540,"elapsed":23637,"user":{"displayName":"Heeseon Im","userId":"07947113443531630188"}},"outputId":"c0248c95-c482-48f7-e3e0-209e54581f41"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.4)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.6.2)\n"]}]},{"cell_type":"code","source":["import torch\n","from transformers import pipeline"],"metadata":{"id":"_XD0lXVIkw6Y","executionInfo":{"status":"ok","timestamp":1720664183561,"user_tz":-540,"elapsed":12347,"user":{"displayName":"Heeseon Im","userId":"07947113443531630188"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["* https://huggingface.co/heegyu/kogpt-j-base 이용"],"metadata":{"id":"-ujNmVOZlLx6"}},{"cell_type":"code","source":["model_name = \"heegyu/kogpt-j-base\"\n","# pipline(): 허깅페이스의 Transformers 라이브러리에서 제공하는\n","# 다양한 자연어 처리 작업을 간편하게 수행할 수 있도록 도와주는 함수\n","pipe = pipeline('text-generation', model=model_name)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":241,"referenced_widgets":["7294103562ec4613b8f9d4d3dda94c26","ec300372586345daacf736657c832d74","8a0fe648df2e4e92a3c51f66b1b9be2c","44cdb129953349ef81ef782ad075f002","b217b545a5de4f2ebb065a2bc8a0a26c","38d3081e402d4ec69320e18a74b4cdba","9c31bf54d84b4211b7073d536961b676","f322a0e0e571419992e3feeca1707b0a","be74537caa8641e7b7f331a55f094e04","4e1a1513c51d42ffbd4d145ab9143ed2","a301bd841894477985c6a370673a5605","d07852437f5543bf97e5bbb5515d0370","707b7505b48942109fa3bf7d4bfab86d","b431f21827074bce98e6c9c56606374d","58839ff78c5641b09cfed6eebb687828","1911d5d08d7a4c9ea6ef76e509daa524","e6befaaeac6e4c7da14f4ebe702f7da9","3de058501bbe4e4ead92117e6a028fc3","8841a0622b4244b3a27bc3f5320d4ea2","28f8681957154148a26105a7e92ec706","e1f17b0c06e04568aa987885ef935b56","54021041ef4f4cbeab18263609019225","bf7302c9a25c432da1b9937a5211581e","112a90ece83840658ab57e23a33d3178","22ca77bc6252488381194e634bd7ad01","6ff036256c5c4462beb0d01ac8137725","fbe8a4b73c4a490d9444370731a8a44a","c0ed08a1c3b146b091323ed1b87bbc8b","fe6f474f8f2641039af62f0d34af186e","fe76ff5bd4dd48439cf307d1891d0c11","633eb56c2b08466d825276eb04785fd5","2ffcb5b2cb3249dab48cb20c14ab3862","82463fd0e79c4c158d3dec72b3373ab7","22af8692326a47c190727f6f2a61deb2","76afa799c9bf497f8b63314e198c9902","233a1ab3bd344b249a631b30b3e7692d","047bd53bbc9f4f0c85d29bc5f2462c72","659679c085ba4c0da055c43313d3d34d","622cc4ff64664c8cb6361d8357db49ed","5ba4288315574269a6e4ac710c3b4ae8","7fff864bb1094e98ace0851088cbfabf","f39f5dff34bb46f18554788501b50f08","67b4f78c51b14f32b013fb1d0fb28ffa","3724d90edf36435b97f5043c8291c5a2","b0242fa532534059aad84fe362718bf5","40896fbead00428eb859ffe2225ed629","37a1f35dc98f45b0b7c5faa01e720700","e5a0a716e89d420984fb636aa3b4f990","4a7af1926780442da2aa8e64ff49ebb3","a23299ee6d9043ef8b64725185a2e25e","7b4b7078766b45739e9f4c6de4bc956a","292d94cc6dc4472c9a74d2adf93b8e1b","00cf53a76f41403caab7d0aa0786afa8","17cc9b07a77e4a208efe1f8328cd76cf","f1c3da6327684a01b16f3f59c2d85e0f","df1c0c18aef648ce93e27fdb6d8ce44f","fdd8eb5b348c484182f83d0908f92673","0e7d1fa105eb4ecd979b7cbd9e4bc378","40efd0b097cf400aac321a3cdb3c7c02","9ff44bb3838b4d3db31dbdd853e7a321","17fa1c5d971a4d4dad74b4ff6afcd769","a6fdd4bfc2414d53b639e2dadae7ee3b","6b70bda8384645f1b407aae4d4ff7452","820c65dc02dc43eb80a06b65d16acf56","c8dc933c5d7b4fb4a327f0daf6ccf7cc","66ed40b8379f4f5a84b09a5291b36ce9","aec93cb16b84497892c9feb35198d000","dee02e5b80ee4cb0bf6678e0cd6af567","77e2ec64d1d04e98b2ae8a65c3e42aee","07d86920917f466da3c6c55c11b9af26","94cb896d0d844b809239426863584f29","83742ecaf1564233b70509695e9c3320","5544b2fae7274cf8bb0c947965a23a9e","03dc7c4dd0254caa84215096a1a786dc","1a5e73cc1c2e49f786eecf360284d605","59e936ced9544af3b764e94bc2cf7120","bb9cdaa4040848e58f62d8624fe0b2d5"]},"id":"xAxW9of9lmLT","executionInfo":{"status":"ok","timestamp":1720664415420,"user_tz":-540,"elapsed":52436,"user":{"displayName":"Heeseon Im","userId":"07947113443531630188"}},"outputId":"c438d8af-8001-43bc-cabd-50d4b1ad8d29"},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/872 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7294103562ec4613b8f9d4d3dda94c26"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["pytorch_model.bin:   0%|          | 0.00/667M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d07852437f5543bf97e5bbb5515d0370"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/790 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf7302c9a25c432da1b9937a5211581e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.json:   0%|          | 0.00/1.27M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"22af8692326a47c190727f6f2a61deb2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["merges.txt:   0%|          | 0.00/925k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b0242fa532534059aad84fe362718bf5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/3.07M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df1c0c18aef648ce93e27fdb6d8ce44f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["special_tokens_map.json:   0%|          | 0.00/96.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aec93cb16b84497892c9feb35198d000"}},"metadata":{}}]},{"cell_type":"code","source":["# do_sample=True\n","print(pipe(\"안녕하세요\", repetition_penalty=1.2, do_sample=True, eos_token_id=1, early_stopping=True, max_new_tokens=128))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RQ3oCudqmVw6","executionInfo":{"status":"ok","timestamp":1720664650521,"user_tz":-540,"elapsed":12543,"user":{"displayName":"Heeseon Im","userId":"07947113443531630188"}},"outputId":"59b19a03-9542-4e03-8da7-1de2c4b55960"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["[{'generated_text': \"안녕하세요 전학을 위해 2년 동안 서울지역 외 거주자 등록을 했다'며 '서울에 거주하고 주민등록이 경기도 안산시에 소재한 초등학교 1학급인 경우는 예외적으로 만10세 자녀는 부모의 주소지 관할동사무소와 주민센터에만 신청하라 고 공문을 발송했다. 일부 지역은 올해 하반기부터는 경기도에서 경기도시부로 전입신고하고 서울시 관내를 관할하는 학교장으로부터 학생등록부 및 가정통신문자를 받아 주소지로 제출하도록 했다'고 말했다. 그러면서 '(경남도교육청)관계자의 경우 주소지 제출을 거부하는 것이 아니라 주소지를 그대로 두고 주민등록초본 등을 발급받은 후 다시 주소를 옮길 계획을 수립했다고 통보\"}]\n"]}]},{"cell_type":"markdown","source":["* repetition_penalty: 텍스트 생성 과정에서 반복되는 단어 또는 구분의 생성을 억제하기 위한 파라미터\n","    * 특정 단어가 반복될 때 단어의 확률을 감소시키는 방식으로 작동\n","    * 모델이 동일한 단어를 다시 생성하려고 할 때 로그 확률에 페널티를 부여하여 다른 단어를 선택하도록 유도\n","    * 1(페널티를 주지 않음)보다 큰 값을 사용, 1.5가 강한 페널티(텍스트 균형이 맞지 않을 수 있음)\n","    * 특정 단어의 원래 확률이 P라면 반복될 때 확률은 P / 1.2로 줄어듬\n","    * 예) 오늘 날씨는 좋습니다. 오늘 날씨는 맑습니다. => 오늘 날씨는 좋습니다. 하늘은 맑고, 기온은 따뜻합니다.\n"],"metadata":{"id":"bzm3EBB7ms8x"}},{"cell_type":"markdown","source":["* do_sample: 텍스트 생성 과정에서 생플링 방법을 설정. True일 경우 모델은 확률 분포에서 토근을 무작위로 선택함. 텍스트 생성에 다양성과 창의성을 더할 수 있음\n","    * 샘플링(Sampling)\n","        * 모델이 예측한 확률 분포에서 무작위로 토큰을 선택하는 방식\n","        * 다양한 결과를 생성할 수 있으며, 예측할 수 없는 창의적인 텍스트를 생성\n","        * 품질이 일관되지 않을 수 있음. 엉뚱하거나 의미없는 결과를 생성할 가능성이 있음\n","    * 빔 서치(Beam Search)\n","        * 여러 경로를 동시에 고려하여 가장 높은 점수를 가진 경로를 선택하는 방식\n","        * 주어진 빔 폭 내에서 가장 가능성 높은 몇 가지 경로를 추적하며, 최종적으로 가장 점수가 높은 경로를 선택\n","        * 일관되고 논리적인 텍스트를 생성할 수 있음\n","        * 덜 창의적이고 반복적인 텍스트를 생성할 수 있음. 계산 비용이 높음\n","    * 샘플링의 세부 설정\n","        * temperature\n","            * 확률 분포를 변화시켜 예측된 확률값을 부드럽게 하거나 날카롭게 만듬\n","            * 높은 값(예: 1.5): 확률 분포를 평탄하게 만들어 무작위성이 높아짐\n","            * 낮은 값(예: 0.6): 확률 분포를 날카롭게 만들어 결정론적으로 만듦\n","        * top-k 샘플링\n","            * 확률 분포에서 상위 k개의 후보만 고려하는 방법\n","            * 상위 k개의 후보 토큰만 남기고 나머지는 무시한 후, 해당 사이에서 샘플링\n","        * top-p 샘플링\n","            * 누적 활률이 p이상이 되는 후보군을 고려하는 방법\n","            * 후보 토큰을 누적 확률이 p가 되는 지점까지 포함시킨 후, 해당 사이에서 샘플링"],"metadata":{"id":"vk0qY6QooVrL"}},{"cell_type":"code","source":["# do_sample=False, repetition_penalty=1.2 => 단어 너무 반복됨ㅠ\n","print(pipe(\"안녕하세요\", repetition_penalty=1.2, do_sample=False, eos_token_id=1, early_stopping=True, max_new_tokens=128))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ewo_YegQru2U","executionInfo":{"status":"ok","timestamp":1720665989175,"user_tz":-540,"elapsed":13924,"user":{"displayName":"Heeseon Im","userId":"07947113443531630188"}},"outputId":"dc2ffb93-bbf2-44f2-cbde-81411d78945b"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["[{'generated_text': '안녕하세요.\\n네.\\n어~ 이~ 그~ 어~ 이~ 저~ 이~ 뭐~ 이~ 이~ 이~ 이~ 이~ 이~ 이~ 이~ 이~ 이~ 이~ 이~ 이~ 이~ 이~ 이~ 이~ 이~ 이~ 이~ 이~ 이~ 이~ 이~ 이~ 이~ 이~ 이~ 이~ 이~ 이~ 이~ 이~ 이~ 이~ 이~ 이~ 이~ 이~ 이~ 이~ 이~ 이~ 이~ 이~ 이~ 이~ 이~ 이~ 이~ 이~ 이~ 이~ 이'}]\n"]}]},{"cell_type":"code","source":["# do_sample=False, repetition_penalty=1.5 => 1.2에서 1.5로 올리면 반복되는걸 줄일 수 있음\n","print(pipe(\"안녕하세요\", repetition_penalty=1.5, do_sample=False, eos_token_id=1, early_stopping=True, max_new_tokens=128))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2-yO8kTiq6Pi","executionInfo":{"status":"ok","timestamp":1720665911377,"user_tz":-540,"elapsed":10467,"user":{"displayName":"Heeseon Im","userId":"07947113443531630188"}},"outputId":"dfd911cf-a44e-482b-f8d0-95f03d85090f"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["[{'generated_text': '안녕하세요.\\n네~ 네. 오늘 첫 번째 소식입니다. 이~ 문재인 대통령이 어제 신년사를 발표했는데요. 어~ 문 대통령은 이제 우리 사회가 직면한 가장 큰 도전은 경제라고 강조했습니다마는 그중에서 특히 소득 불평등 문제를 집중적으로 거론하면서 양극화 해소를 위한 노력을 기울여 나가겠다 라고 밝혔습니다만 과연 어떤 내용이 담길지 주목됩니다. 자 먼저 일 분 뒤에 바로 이어서는 지금 현재 한국 사회에서 벌어지고 있는 여러 가지 사회 문제들을 짚어보겠습니다. 아시다시피 이천십오 년 사 월 십팔 일에 있었던 촛불 집회가 있었습니다만은 당시 박근혜 대통령 탄핵에 대한 국민적 열망이 컸고'}]\n"]}]},{"cell_type":"code","source":["print(pipe(\"오늘 정부 발표에 따르면, \", repetition_penalty=1.2, do_sample=True, eos_token_id=1, early_stopping=True, max_new_tokens=128))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JhuXskdUr4bS","executionInfo":{"status":"ok","timestamp":1720666021618,"user_tz":-540,"elapsed":11907,"user":{"displayName":"Heeseon Im","userId":"07947113443531630188"}},"outputId":"f2df34a2-3c7d-4ed9-d736-85e5c9310acc"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["[{'generated_text': \"오늘 정부 발표에 따르면, iptv는 월정액 가입자를 기준으로 최대 7만1000원까지 요금을 할인해 준다. 다만 요금 할인 폭은 1~3만원 선으로 제한된다.. 최근 모바일 광고시장이 (이름)상거래 중심으로 성장하면서 국내 iptv 역시 '콘텐츠가 있는 곳이 어디인가'라는 인식을 기반으로 서비스를 시작한 것이 특징이다. 하지만 콘텐츠 제공사 선정과 이용자 확대가 쉽지만은 않은 상(이름)고 설명했다.. 업계에 의하면 국내 인터넷 동영상 앱 시장의 성장은 pc나 스마트폰뿐 아니라 태블릿pc에도 일어났다.. 시장조사기관 유비벨록에 따르면, 2015년 4분기 기준 3세대 이상 스마트폰의 전체 트래픽은\"}]\n"]}]},{"cell_type":"code","source":["print(pipe(\"싸늘하다. 가슴에 비수가 날아와 꽂힌다. \", repetition_penalty=1.2, do_sample=True, eos_token_id=1, early_stopping=True, max_new_tokens=128, min_length=64))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YQzVCtHhr9JS","executionInfo":{"status":"ok","timestamp":1720666073689,"user_tz":-540,"elapsed":12316,"user":{"displayName":"Heeseon Im","userId":"07947113443531630188"}},"outputId":"1d1a914f-ef9e-4dd2-8c81-1c54efa61861"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["[{'generated_text': '싸늘하다. 가슴에 비수가 날아와 꽂힌다. \\n지성이라 칭한 그 사람은 천오백 년 전 어떤 시대에 살고 있을까. 지금은 어느 시대를 가나 늘 풍류의 시대와 만난다. 옛 선비들은 이 시대의 문학은 기이한 의지로 창조된 것이며, 그런 창조는 모든 삶의 원천이나 지성에 의해 실현된다. 기이한 것을 보면 왜 그렇게 하는 것일까에 대해 질문하고 그 해답을 찾으려고 노력하는 데서 새로운 것을 찾아야 한다고 생각했다라고 하였다.\\n그런데 『이수전집』에는 이런 말이 없다. 나는 이제 내가 글을 못 쓰는구나.\" 하고 탄식한다. 다시 말하지만 『이수전집』은 창작의 가장 중요한 책 한 권의 제목이'}]\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"GtkZfb_Ur9M1"},"execution_count":null,"outputs":[]}]}